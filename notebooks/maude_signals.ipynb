{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5abbdee-94bc-49f9-a760-e6fe0b669f4e",
   "metadata": {},
   "source": [
    "This cell initializes the project paths and configuration so the notebook runs reliably on any machine. It first resolves the raw MAUDE data directory (preferring the MAUDE_RAW environment variable, with a Mac fallback), then derives standardized output folders for tables and figures and verifies they’re writable. It records key analysis parameters—time window, intervention quarter, plotting limits, and reproducibility seed—so later cells don’t hard-code values. The cell inventories expected ZIP files (devices, narratives, events, codes) and prints any missing items to catch setup problems early. Finally, it saves a lightweight config.json with paths, parameters, and Python version to document the environment and support reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531f1992-56bc-4f6d-ab40-8494d0cf32fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] OS=Linux 6.8.0-87-generic | Python=3.11.14\n",
      "[paths] RAW_DIR: /home/parallels/data/capstone-maude/Data/raw/Maude\n",
      "[inv] Found ZIPs (11):\n",
      " - device2021.zip\n",
      " - device2022.zip\n",
      " - device2023.zip\n",
      " - device2024.zip\n",
      " - deviceproblemcodes.zip\n",
      " - foidevproblem.zip\n",
      " - foitext2021.zip\n",
      " - foitext2022.zip\n",
      " - foitext2023.zip\n",
      " - foitext2024.zip\n",
      " - mdrfoithru2024.zip\n",
      "\n",
      "Missing: none\n",
      "\n",
      "Write check: OK (/home/parallels/data/capstone-maude/Data/outputs/tables)\n",
      "\n",
      "Saved config → /home/parallels/data/capstone-maude/Data/outputs/config.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell 01: Config & Paths (cross-platform, hardened) ===\n",
    "from pathlib import Path\n",
    "import json, os, sys, platform\n",
    "from datetime import datetime\n",
    "\n",
    "# --- RAW DIR: env first, then Mac fallback ---\n",
    "RAW_DIR = Path(os.environ.get(\n",
    "    \"MAUDE_RAW\",\n",
    "    \"/Users/michaelmohle/Desktop/DS_archive/WGU/D502/capstone-maude/Data/raw/Maude\"\n",
    ")).expanduser().resolve()\n",
    "\n",
    "assert RAW_DIR.exists(), f\"RAW_DIR not found: {RAW_DIR}\"\n",
    "\n",
    "# --- Project/output dirs ---\n",
    "PROJECT_ROOT = RAW_DIR.parent.parent\n",
    "OUT_ROOT = PROJECT_ROOT / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_FIG  = OUT_ROOT / \"figures\"\n",
    "for p in (OUT_ROOT, OUT_TAB, OUT_FIG): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Analysis params in one place ---\n",
    "PARAMS = dict(\n",
    "    WINDOW_START=\"2021-Q1\",\n",
    "    WINDOW_END=\"2024-Q4\",\n",
    "    BREAK_QUARTER=\"2024-Q1\",\n",
    "    TARGET_INDEX=0,\n",
    "    N_TOP_BRANDS=12,\n",
    "    RANDOM_SEED=42,\n",
    "    STRICT_ZIP_CHECK=False,  # set True to hard-fail if any expected ZIP missing\n",
    ")\n",
    "\n",
    "# --- Expected raw files ---\n",
    "expected_files = {\n",
    "    \"device2021.zip\",\"device2022.zip\",\"device2023.zip\",\"device2024.zip\",\n",
    "    \"deviceproblemcodes.zip\",\"foidevproblem.zip\",\n",
    "    \"foitext2021.zip\",\"foitext2022.zip\",\"foitext2023.zip\",\"foitext2024.zip\",\n",
    "    \"mdrfoithru2024.zip\",\n",
    "}\n",
    "\n",
    "# --- Inventory & checks ---\n",
    "found = {p.name for p in RAW_DIR.glob(\"*.zip\")}\n",
    "missing = sorted(expected_files - found)\n",
    "extra   = sorted(found - expected_files)\n",
    "\n",
    "print(f\"[env] OS={platform.system()} {platform.release()} | Python={sys.version.split()[0]}\")\n",
    "print(f\"[paths] RAW_DIR: {RAW_DIR}\")\n",
    "print(f\"[inv] Found ZIPs ({len(found)}):\")\n",
    "for name in sorted(found): print(\" -\", name)\n",
    "print(\"\\nMissing:\", \"none\" if not missing else \"\")\n",
    "for m in missing: print(\" -\", m)\n",
    "if extra:\n",
    "    print(\"\\nNote: extra ZIPs present (not in expected list):\")\n",
    "    for e in extra: print(\" -\", e)\n",
    "if PARAMS[\"STRICT_ZIP_CHECK\"] and missing:\n",
    "    raise FileNotFoundError(f\"Missing required ZIPs: {missing}\")\n",
    "\n",
    "# --- Write check ---\n",
    "(OUT_TAB / \"_write_check.tmp\").write_text(f\"ok {datetime.now().isoformat()}\")\n",
    "(OUT_TAB / \"_write_check.tmp\").unlink(missing_ok=True)\n",
    "print(f\"\\nWrite check: OK ({OUT_TAB})\")\n",
    "\n",
    "# --- Save config (paths + params) ---\n",
    "config = {\n",
    "    \"paths\": {\n",
    "        \"RAW_DIR\": str(RAW_DIR),\n",
    "        \"PROJECT_ROOT\": str(PROJECT_ROOT),\n",
    "        \"OUT_ROOT\": str(OUT_ROOT),\n",
    "        \"OUT_TAB\": str(OUT_TAB),\n",
    "        \"OUT_FIG\": str(OUT_FIG),\n",
    "    },\n",
    "    \"params\": PARAMS,\n",
    "    \"env\": {\"python\": sys.version.split()[0]},\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "(OUT_ROOT / \"config.json\").write_text(json.dumps(config, indent=2))\n",
    "print(f\"\\nSaved config → {OUT_ROOT / 'config.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970d13c-fe1e-4256-b6f1-6d58bfecfbc4",
   "metadata": {},
   "source": [
    "This cell inspects each MAUDE ZIP to map what’s inside and pick the right file to parse later. It lists every member (name and uncompressed size) and saves that catalog to raw_inventory.csv for transparency and audit. For each ZIP, it then chooses the largest plausible data member (skipping readme/license files), guesses the delimiter (pipe for MAUDE text, comma for the problem-code dictionary), and writes the selections to chosen_members.csv. This preselection step prevents accidental parsing of tiny documentation files, standardizes inputs across environments, and gives downstream cells a deterministic, reproducible reference to stream-read only the necessary columns from the correct source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d05a16f-5a9f-4fe3-88a3-208852086552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inventory → /home/parallels/data/capstone-maude/Data/outputs/tables/raw_inventory.csv\n",
      "Saved choices   → /home/parallels/data/capstone-maude/Data/outputs/tables/chosen_members.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>member</th>\n",
       "      <th>uncomp_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>device2021.zip</td>\n",
       "      <td>DEVICE2021.txt</td>\n",
       "      <td>470614592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device2022.zip</td>\n",
       "      <td>DEVICE2022.txt</td>\n",
       "      <td>679452440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>device2023.zip</td>\n",
       "      <td>DEVICE2023.txt</td>\n",
       "      <td>548798875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>device2024.zip</td>\n",
       "      <td>DEVICE2024.txt</td>\n",
       "      <td>618911446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deviceproblemcodes.zip</td>\n",
       "      <td>deviceproblemcodes.csv</td>\n",
       "      <td>33815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>foidevproblem.zip</td>\n",
       "      <td>foidevproblem.txt</td>\n",
       "      <td>325790429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>foitext2021.zip</td>\n",
       "      <td>foitext2021.txt</td>\n",
       "      <td>2346985867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>foitext2022.zip</td>\n",
       "      <td>foitext2022.txt</td>\n",
       "      <td>3396365320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>foitext2023.zip</td>\n",
       "      <td>foitext2023.txt</td>\n",
       "      <td>3291484171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>foitext2024.zip</td>\n",
       "      <td>foitext2024.txt</td>\n",
       "      <td>3278869050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mdrfoithru2024.zip</td>\n",
       "      <td>mdrfoiThru2024.txt</td>\n",
       "      <td>6966021112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       zip                  member  uncomp_size\n",
       "0           device2021.zip          DEVICE2021.txt    470614592\n",
       "1           device2022.zip          DEVICE2022.txt    679452440\n",
       "2           device2023.zip          DEVICE2023.txt    548798875\n",
       "3           device2024.zip          DEVICE2024.txt    618911446\n",
       "4   deviceproblemcodes.zip  deviceproblemcodes.csv        33815\n",
       "5        foidevproblem.zip       foidevproblem.txt    325790429\n",
       "6          foitext2021.zip         foitext2021.txt   2346985867\n",
       "7          foitext2022.zip         foitext2022.txt   3396365320\n",
       "8          foitext2023.zip         foitext2023.txt   3291484171\n",
       "9          foitext2024.zip         foitext2024.txt   3278869050\n",
       "10      mdrfoithru2024.zip      mdrfoiThru2024.txt   6966021112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>chosen_member</th>\n",
       "      <th>chosen_size_bytes</th>\n",
       "      <th>guessed_delimiter</th>\n",
       "      <th>zip_md5_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>device2021.zip</td>\n",
       "      <td>DEVICE2021.txt</td>\n",
       "      <td>470614592</td>\n",
       "      <td>|</td>\n",
       "      <td>79d756f8aedb2863581859811ff0de80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device2022.zip</td>\n",
       "      <td>DEVICE2022.txt</td>\n",
       "      <td>679452440</td>\n",
       "      <td>|</td>\n",
       "      <td>046e1e5294436ab247ec9887d1752ace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>device2023.zip</td>\n",
       "      <td>DEVICE2023.txt</td>\n",
       "      <td>548798875</td>\n",
       "      <td>|</td>\n",
       "      <td>2f96d01c70f2bccefeb99aacdbd62547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>device2024.zip</td>\n",
       "      <td>DEVICE2024.txt</td>\n",
       "      <td>618911446</td>\n",
       "      <td>|</td>\n",
       "      <td>507eda80e1da0c7ef0aec1cd45ddc2e6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deviceproblemcodes.zip</td>\n",
       "      <td>deviceproblemcodes.csv</td>\n",
       "      <td>33815</td>\n",
       "      <td>,</td>\n",
       "      <td>7fa172423dd2a2e4d23d69747baf482d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>foidevproblem.zip</td>\n",
       "      <td>foidevproblem.txt</td>\n",
       "      <td>325790429</td>\n",
       "      <td>|</td>\n",
       "      <td>86ff105e7afde7c0f62fa5d8c3e513f6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>foitext2021.zip</td>\n",
       "      <td>foitext2021.txt</td>\n",
       "      <td>2346985867</td>\n",
       "      <td>|</td>\n",
       "      <td>20ec8d0f2396378f573f20c68230c690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>foitext2022.zip</td>\n",
       "      <td>foitext2022.txt</td>\n",
       "      <td>3396365320</td>\n",
       "      <td>|</td>\n",
       "      <td>aa5854960754ec7bef8a8c12ef81cb02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>foitext2023.zip</td>\n",
       "      <td>foitext2023.txt</td>\n",
       "      <td>3291484171</td>\n",
       "      <td>|</td>\n",
       "      <td>a4b60c00d012056304082fd41e24bec9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>foitext2024.zip</td>\n",
       "      <td>foitext2024.txt</td>\n",
       "      <td>3278869050</td>\n",
       "      <td>|</td>\n",
       "      <td>893506f0ef400edd3057c2ca7b3b2edd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mdrfoithru2024.zip</td>\n",
       "      <td>mdrfoiThru2024.txt</td>\n",
       "      <td>6966021112</td>\n",
       "      <td>|</td>\n",
       "      <td>c4fb49d5150109fdeebb800bff866bf7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       zip           chosen_member  chosen_size_bytes  \\\n",
       "0           device2021.zip          DEVICE2021.txt          470614592   \n",
       "1           device2022.zip          DEVICE2022.txt          679452440   \n",
       "2           device2023.zip          DEVICE2023.txt          548798875   \n",
       "3           device2024.zip          DEVICE2024.txt          618911446   \n",
       "4   deviceproblemcodes.zip  deviceproblemcodes.csv              33815   \n",
       "5        foidevproblem.zip       foidevproblem.txt          325790429   \n",
       "6          foitext2021.zip         foitext2021.txt         2346985867   \n",
       "7          foitext2022.zip         foitext2022.txt         3396365320   \n",
       "8          foitext2023.zip         foitext2023.txt         3291484171   \n",
       "9          foitext2024.zip         foitext2024.txt         3278869050   \n",
       "10      mdrfoithru2024.zip      mdrfoiThru2024.txt         6966021112   \n",
       "\n",
       "   guessed_delimiter                      zip_md5_head  \n",
       "0                  |  79d756f8aedb2863581859811ff0de80  \n",
       "1                  |  046e1e5294436ab247ec9887d1752ace  \n",
       "2                  |  2f96d01c70f2bccefeb99aacdbd62547  \n",
       "3                  |  507eda80e1da0c7ef0aec1cd45ddc2e6  \n",
       "4                  ,  7fa172423dd2a2e4d23d69747baf482d  \n",
       "5                  |  86ff105e7afde7c0f62fa5d8c3e513f6  \n",
       "6                  |  20ec8d0f2396378f573f20c68230c690  \n",
       "7                  |  aa5854960754ec7bef8a8c12ef81cb02  \n",
       "8                  |  a4b60c00d012056304082fd41e24bec9  \n",
       "9                  |  893506f0ef400edd3057c2ca7b3b2edd  \n",
       "10                 |  c4fb49d5150109fdeebb800bff866bf7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 02 Inventory & Member Selection ===\n",
    "from pathlib import Path\n",
    "import zipfile, re, os, hashlib\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path(os.environ.get(\"MAUDE_RAW\", \"\")).expanduser().resolve()\n",
    "assert RAW_DIR.exists(), f\"RAW_DIR not found: {RAW_DIR}\"\n",
    "\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_TAB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def guess_delim(zipname: str, member: str) -> str:\n",
    "    zn, mn = zipname.lower(), member.lower()\n",
    "    if \"deviceproblemcodes\" in zn or mn.endswith(\".csv\"):\n",
    "        return \",\"\n",
    "    # a few rare CSV shipments in foidevproblem; default to pipe otherwise\n",
    "    if \"foidevproblem\" in zn and mn.endswith(\".csv\"):\n",
    "        return \",\"\n",
    "    return \"|\"\n",
    "\n",
    "def prefer_member(infos):\n",
    "    \"\"\"Prefer canonical names; otherwise largest text file (skip readme/license).\"\"\"\n",
    "    files = [i for i in infos if not i.is_dir()]\n",
    "    # canonical patterns\n",
    "    order = [\n",
    "        r\"^mdrfoi.*\\.txt$\",      # e.g., mdrfoiThru2024.txt\n",
    "        r\"^device20\\d{2}\\.txt$\", # DEVICE2021.txt ...\n",
    "        r\"^foitext20\\d{2}\\.txt$\",\n",
    "        r\"^foidevproblem\\.txt$\",\n",
    "        r\"^deviceproblemcodes\\.csv$\",\n",
    "        r\".*\\.(txt|csv)$\"\n",
    "    ]\n",
    "    for pat in order:\n",
    "        cand = [i for i in files\n",
    "                if re.search(pat, i.filename, re.I)\n",
    "                and not re.search(r\"(readme|license|doc)\", i.filename, re.I)]\n",
    "        if cand:\n",
    "            return max(cand, key=lambda i: i.file_size)\n",
    "    return None\n",
    "\n",
    "def md5sum(path: Path, n=1024*1024):\n",
    "    # quick md5 over first n bytes of the zip for traceability (optional)\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        h.update(f.read(n))\n",
    "    return h.hexdigest()\n",
    "\n",
    "rows_inv, rows_choice = [], []\n",
    "for zp in sorted(RAW_DIR.glob(\"*.zip\")):\n",
    "    with zipfile.ZipFile(zp) as zf:\n",
    "        infos = zf.infolist()\n",
    "        for inf in infos:\n",
    "            if inf.is_dir():  # inventory: skip pseudo-dirs\n",
    "                continue\n",
    "            rows_inv.append({\n",
    "                \"zip\": zp.name,\n",
    "                \"member\": inf.filename,\n",
    "                \"uncomp_size\": inf.file_size,\n",
    "            })\n",
    "        picked = prefer_member(infos)\n",
    "        if picked:\n",
    "            rows_choice.append({\n",
    "                \"zip\": zp.name,\n",
    "                \"chosen_member\": picked.filename,\n",
    "                \"chosen_size_bytes\": picked.file_size,\n",
    "                \"guessed_delimiter\": guess_delim(zp.name, picked.filename),\n",
    "                \"zip_md5_head\": md5sum(zp),  # optional provenance\n",
    "            })\n",
    "        else:\n",
    "            rows_choice.append({\n",
    "                \"zip\": zp.name,\n",
    "                \"chosen_member\": \"\",\n",
    "                \"chosen_size_bytes\": 0,\n",
    "                \"guessed_delimiter\": \"\",\n",
    "                \"zip_md5_head\": md5sum(zp),\n",
    "            })\n",
    "\n",
    "inv = pd.DataFrame(rows_inv).sort_values([\"zip\",\"uncomp_size\"], ascending=[True,False])\n",
    "choices = pd.DataFrame(rows_choice)\n",
    "\n",
    "inv_path = OUT_TAB / \"raw_inventory.csv\"\n",
    "ch_path  = OUT_TAB / \"chosen_members.csv\"\n",
    "inv.to_csv(inv_path, index=False)\n",
    "choices.to_csv(ch_path, index=False)\n",
    "\n",
    "print(\"Saved inventory →\", inv_path)\n",
    "print(\"Saved choices   →\", ch_path)\n",
    "display(inv.head(15))\n",
    "display(choices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089502f6-4f7c-4f2f-adb9-c0dbc767d92f",
   "metadata": {},
   "source": [
    "This cell stream-parses the large MAUDE ZIPs into small, analysis-ready CSVs—without loading whole files into memory. For each ZIP/member chosen in Cell 02, we open a text stream with tolerant encodings (UTF-8 → Latin-1 fallback), use a chunked pandas reader, and skip malformed lines safely. Within each chunk we regex-match canonical column names, trim whitespace, and append just the needed fields to compact outputs: devices_min.csv (event_id, brand, model, date_received), events_min.csv (event_id, event_date), narratives_min.csv (event_id, narrative_text), foidevproblem_min.csv (event_id, problem_code), and deviceproblemcodes_min.csv (code→term dictionary). The result is a reproducible, memory-safe extraction layer that standardizes messy raw inputs into consistent tables for downstream joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db35e01e-b3eb-4598-9406-604c86cf57c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device2021.zip       → +2,032,317 rows into devices_min.csv (append)\n",
      "  …device2022.zip chunks=25 (+69,579 rows last)\n",
      "device2022.zip       → +2,949,579 rows into devices_min.csv (append)\n",
      "device2023.zip       → +2,340,344 rows into devices_min.csv (append)\n",
      "device2024.zip       → +2,627,159 rows into devices_min.csv (append)\n",
      "deviceproblemcodes.zip → +882 rows into deviceproblemcodes_min.csv\n",
      "  …foidevproblem.zip chunks=25 (+0 rows last)\n",
      "  …foidevproblem.zip chunks=50 (+0 rows last)\n",
      "  …foidevproblem.zip chunks=75 (+0 rows last)\n",
      "  …foidevproblem.zip chunks=100 (+0 rows last)\n",
      "  …foidevproblem.zip chunks=125 (+0 rows last)\n",
      "  …foidevproblem.zip chunks=150 (+0 rows last)\n",
      "  …foidevproblem.zip chunks=175 (+0 rows last)\n",
      "foidevproblem.zip    → +0 rows into foidevproblem_min.csv\n",
      "[INFO] Skipping foitext2021.zip (toggle)\n",
      "[INFO] Skipping foitext2022.zip (toggle)\n",
      "[INFO] Skipping foitext2023.zip (toggle)\n",
      "[INFO] Skipping foitext2024.zip (toggle)\n",
      "  …mdrfoithru2024.zip chunks=25 (+120,000 rows last)\n",
      "  …mdrfoithru2024.zip chunks=50 (+120,000 rows last)\n",
      "  …mdrfoithru2024.zip chunks=75 (+120,000 rows last)\n",
      "  …mdrfoithru2024.zip chunks=100 (+120,000 rows last)\n",
      "  …mdrfoithru2024.zip chunks=125 (+120,000 rows last)\n",
      "  …mdrfoithru2024.zip chunks=150 (+120,000 rows last)\n",
      "mdrfoithru2024.zip   → +20,746,959 rows into events_min.csv\n",
      "\n",
      "Row counts by source:\n",
      " - device2021.zip            2,032,317\n",
      " - device2022.zip            2,949,579\n",
      " - device2023.zip            2,340,344\n",
      " - device2024.zip            2,627,159\n",
      " - deviceproblemcodes.zip          882\n",
      " - foidevproblem.zip                 0\n",
      " - mdrfoithru2024.zip       20,746,959\n",
      "\n",
      "Outputs:\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/devices_min.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/narratives_min.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/events_min.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/foidevproblem_min.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/deviceproblemcodes_min.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell 03 (VM-friendly): Stream MAUDE zips → light CSVs (long running) ====================\n",
    "# Outputs (under Data/outputs/tables):\n",
    "#   devices_min.csv           [event_id, brand, model, date_received]\n",
    "#   narratives_min.csv        [event_id, narrative_text]\n",
    "#   events_min.csv            [event_id, event_date]\n",
    "#   foidevproblem_min.csv     [event_id, problem_code]\n",
    "#   deviceproblemcodes_min.csv[FDA_CODE, TERM]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, zipfile, csv, io, sys\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Paths ----------\n",
    "RAW_DIR = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "assert RAW_DIR.exists(), f\"RAW_DIR not found: {RAW_DIR}\"\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_TAB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "choices_path = OUT_TAB / \"chosen_members.csv\"\n",
    "assert choices_path.exists(), f\"Missing {choices_path} (run Cell 02 first).\"\n",
    "choices = pd.read_csv(choices_path)\n",
    "\n",
    "p_devices  = OUT_TAB / \"devices_min.csv\"\n",
    "p_texts    = OUT_TAB / \"narratives_min.csv\"\n",
    "p_events   = OUT_TAB / \"events_min.csv\"\n",
    "p_dpm      = OUT_TAB / \"foidevproblem_min.csv\"\n",
    "p_codes    = OUT_TAB / \"deviceproblemcodes_min.csv\"\n",
    "\n",
    "# Remove any previous partials to avoid duplicate appends\n",
    "for p in (p_devices, p_texts, p_events, p_dpm, p_codes):\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "\n",
    "# ---------- Speed/Memory Toggles ----------\n",
    "PROCESS_ONLY = {\n",
    "    \"devices\": True,     # device20xx.zip\n",
    "    \"events\":  True,     # mdrfoithru2024.zip\n",
    "    \"narratives\": False, # foitext20xx.zip  (turn True in a second pass if needed)\n",
    "    \"devproblem\": True,  # foidevproblem.zip\n",
    "    \"dict\": True,        # deviceproblemcodes.zip\n",
    "}\n",
    "CHUNK_ROWS   = 120_000     # reduce for lower RAM; increase for speed\n",
    "STATUS_EVERY = 25          # progress print every N chunks\n",
    "POSSIBLE_ENCS = (\"utf-8\", \"latin-1\")\n",
    "\n",
    "# allow very long narrative fields\n",
    "csv.field_size_limit(min(2_000_000_000, sys.maxsize))\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _open_text(zf: zipfile.ZipFile, member: str, enc: str):\n",
    "    return io.TextIOWrapper(\n",
    "        zf.open(member, \"r\"),\n",
    "        encoding=enc,\n",
    "        errors=\"ignore\",\n",
    "        newline=\"\"\n",
    "    )\n",
    "\n",
    "def _reader_from_member(zpath: Path, member: str, sep: str, chunksize: int):\n",
    "    \"\"\"Try encodings until a pandas chunk reader is created.\"\"\"\n",
    "    for enc in POSSIBLE_ENCS:\n",
    "        try:\n",
    "            zf = zipfile.ZipFile(zpath)\n",
    "            txt = _open_text(zf, member, enc)\n",
    "            reader = pd.read_csv(\n",
    "                txt,\n",
    "                sep=sep,\n",
    "                engine=\"python\",             # robust to wonky quotes\n",
    "                dtype=str,\n",
    "                on_bad_lines=\"skip\",\n",
    "                quoting=csv.QUOTE_NONE if sep == \"|\" else csv.QUOTE_MINIMAL,\n",
    "                escapechar=\"\\\\\" if sep == \"|\" else None,\n",
    "                chunksize=chunksize\n",
    "            )\n",
    "            # attach zf so it stays open while iterating\n",
    "            reader._llm_zipfile_ref = zf   # keep reference to avoid GC closing it\n",
    "            return reader\n",
    "        except Exception:\n",
    "            try:\n",
    "                zf.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            continue\n",
    "    raise RuntimeError(\n",
    "        f\"Could not create reader for {zpath.name}:{member} (sep='{sep}') \"\n",
    "        f\"with encodings {POSSIBLE_ENCS}\"\n",
    "    )\n",
    "\n",
    "def _normalize_and_append(df_chunk: pd.DataFrame, target_cols: dict,\n",
    "                          out_path: Path, first_flags: dict, order: list) -> int:\n",
    "    \"\"\"Pick columns via regex names in the chunk, rename to targets, append in order.\"\"\"\n",
    "    rename_map = {}\n",
    "    for tgt, patterns in target_cols.items():\n",
    "        for pat in patterns:\n",
    "            m = [c for c in df_chunk.columns if re.search(pat, str(c), re.I)]\n",
    "            if m:\n",
    "                rename_map[m[0]] = tgt\n",
    "                break\n",
    "    keep = [c for c in df_chunk.columns if c in rename_map]\n",
    "    if not keep:\n",
    "        return 0\n",
    "\n",
    "    sub = df_chunk[keep].rename(columns=rename_map)\n",
    "    for c in sub.columns:\n",
    "        sub[c] = sub[c].astype(str).str.strip()\n",
    "\n",
    "    cols = [c for c in order if c in sub.columns]\n",
    "    sub = sub[cols]\n",
    "\n",
    "    if first_flags.setdefault(out_path, True):\n",
    "        sub.to_csv(out_path, index=False, mode=\"w\")\n",
    "        first_flags[out_path] = False\n",
    "    else:\n",
    "        sub.to_csv(out_path, index=False, header=False, mode=\"a\")\n",
    "    return len(sub)\n",
    "\n",
    "# ---------- Column patterns ----------\n",
    "want_devices = {\n",
    "    \"event_id\":      [r\"^MDR[_ ]?REPORT[_ ]?KEY$\", r\"^REPORT[_ ]?KEY$\"],\n",
    "    \"brand\":         [r\"^BRAND[_ ]?NAME$\"],\n",
    "    \"model\":         [r\"^MODEL[_ ]?NUMBER$\"],\n",
    "    \"date_received\": [r\"^DATE[_ ]?RECEIVED$\", r\"^DATE[_ ]?RPTD[_ ]?TO[_ ]?FDA$\"],\n",
    "}\n",
    "want_texts = {\n",
    "    \"event_id\":       [r\"^MDR[_ ]?REPORT[_ ]?KEY$\", r\"^REPORT[_ ]?KEY$\"],\n",
    "    \"narrative_text\": [r\"^FOI[_ ]?TEXT$\"],\n",
    "}\n",
    "want_events = {\n",
    "    \"event_id\":   [r\"^MDR[_ ]?REPORT[_ ]?KEY$\", r\"^REPORT[_ ]?KEY$\"],\n",
    "    \"event_date\": [r\"^DATE[_ ]?RECEIVED$\", r\"^DATE[_ ]?OF[_ ]?EVENT$\", r\"^REPORT[_ ]?DT$\"],\n",
    "}\n",
    "want_dpm = {\n",
    "    \"event_id\":     [r\"^MDR[_ ]?REPORT[_ ]?KEY$\", r\"^REPORT[_ ]?KEY$\"],\n",
    "    \"problem_code\": [r\"^DEVICE[_ ]?PROBLEM[_ ]?CODE$\", r\"^PROBLEM[_ ]?CODE$\"],\n",
    "}\n",
    "\n",
    "order_devices = [\"event_id\",\"brand\",\"model\",\"date_received\"]\n",
    "order_texts   = [\"event_id\",\"narrative_text\"]\n",
    "order_events  = [\"event_id\",\"event_date\"]\n",
    "order_dpm     = [\"event_id\",\"problem_code\"]\n",
    "\n",
    "# ---------- Small dictionary builder ----------\n",
    "def build_deviceproblemcodes():\n",
    "    row = choices.loc[choices[\"zip\"] == \"deviceproblemcodes.zip\"]\n",
    "    if row.empty:\n",
    "        return 0\n",
    "    zpath  = RAW_DIR / \"deviceproblemcodes.zip\"\n",
    "    member = row.iloc[0][\"chosen_member\"]\n",
    "    with zipfile.ZipFile(zpath) as zf, zf.open(member, \"r\") as fb:\n",
    "        data = fb.read()\n",
    "    text = data.decode(\"utf-8\", errors=\"ignore\")\n",
    "    from io import StringIO\n",
    "    df = pd.read_csv(StringIO(text), dtype=str)\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    code_col = cols.get(\"fda_code\") or cols.get(\"device_problem_code\") or cols.get(\"problem_code\") or df.columns[0]\n",
    "    term_col = cols.get(\"term\") or cols.get(\"problem_code_desc\") or cols.get(\"problem_code_description\") or cols.get(\"device_problem_code_desc\") or df.columns[1]\n",
    "    slim = df[[code_col, term_col]].rename(columns={code_col:\"FDA_CODE\", term_col:\"TERM\"})\n",
    "    slim.to_csv(p_codes, index=False)\n",
    "    return len(slim)\n",
    "\n",
    "# ---------- Run extraction ----------\n",
    "counts = {}\n",
    "first_flags = {}\n",
    "STATUS = STATUS_EVERY\n",
    "\n",
    "for _, row in choices.iterrows():\n",
    "    zipname = row[\"zip\"]\n",
    "    member  = row[\"chosen_member\"]\n",
    "    sep     = (row.get(\"guessed_delimiter\") or \"|\").strip() or \"|\"\n",
    "    zpath   = RAW_DIR / zipname\n",
    "\n",
    "    # Devices\n",
    "    if zipname.startswith(\"device20\") and PROCESS_ONLY[\"devices\"]:\n",
    "        reader = _reader_from_member(zpath, member, sep, CHUNK_ROWS)\n",
    "        total = 0\n",
    "        for i, chunk in enumerate(reader, start=1):\n",
    "            wrote = _normalize_and_append(chunk, want_devices, p_devices, first_flags, order_devices)\n",
    "            total += wrote\n",
    "            if i % STATUS == 0:\n",
    "                print(f\"  …{zipname} chunks={i:,} (+{wrote:,} rows last)\")\n",
    "        counts[zipname] = total\n",
    "        print(f\"{zipname:<20} → +{total:,} rows into {p_devices.name} (append)\")\n",
    "\n",
    "    # Narratives\n",
    "    elif zipname.startswith(\"foitext20\") and PROCESS_ONLY[\"narratives\"]:\n",
    "        reader = _reader_from_member(zpath, member, sep, CHUNK_ROWS)\n",
    "        total = 0\n",
    "        for i, chunk in enumerate(reader, start=1):\n",
    "            wrote = _normalize_and_append(chunk, want_texts, p_texts, first_flags, order_texts)\n",
    "            total += wrote\n",
    "            if i % STATUS == 0:\n",
    "                print(f\"  …{zipname} chunks={i:,} (+{wrote:,} rows last)\")\n",
    "        counts[zipname] = total\n",
    "        print(f\"{zipname:<20} → +{total:,} rows into {p_texts.name}\")\n",
    "\n",
    "    # Events\n",
    "    elif zipname == \"mdrfoithru2024.zip\" and PROCESS_ONLY[\"events\"]:\n",
    "        reader = _reader_from_member(zpath, member, sep, CHUNK_ROWS)\n",
    "        total = 0\n",
    "        for i, chunk in enumerate(reader, start=1):\n",
    "            wrote = _normalize_and_append(chunk, want_events, p_events, first_flags, order_events)\n",
    "            total += wrote\n",
    "            if i % STATUS == 0:\n",
    "                print(f\"  …{zipname} chunks={i:,} (+{wrote:,} rows last)\")\n",
    "        counts[zipname] = total\n",
    "        print(f\"{zipname:<20} → +{total:,} rows into {p_events.name}\")\n",
    "\n",
    "    # Device problem mapping\n",
    "    elif zipname == \"foidevproblem.zip\" and PROCESS_ONLY[\"devproblem\"]:\n",
    "        reader = _reader_from_member(zpath, member, sep, CHUNK_ROWS)\n",
    "        total = 0\n",
    "        for i, chunk in enumerate(reader, start=1):\n",
    "            wrote = _normalize_and_append(chunk, want_dpm, p_dpm, first_flags, order_dpm)\n",
    "            total += wrote\n",
    "            if i % STATUS == 0:\n",
    "                print(f\"  …{zipname} chunks={i:,} (+{wrote:,} rows last)\")\n",
    "        counts[zipname] = total\n",
    "        print(f\"{zipname:<20} → +{total:,} rows into {p_dpm.name}\")\n",
    "\n",
    "    # Problem dictionary (CSV)\n",
    "    elif zipname == \"deviceproblemcodes.zip\" and PROCESS_ONLY[\"dict\"]:\n",
    "        n = build_deviceproblemcodes()\n",
    "        counts[zipname] = n\n",
    "        print(f\"{zipname:<20} → +{n:,} rows into {p_codes.name}\")\n",
    "\n",
    "    else:\n",
    "        # skipped by toggle\n",
    "        print(f\"[INFO] Skipping {zipname} (toggle)\")\n",
    "\n",
    "# ---------- Summary ----------\n",
    "print(\"\\nRow counts by source:\")\n",
    "for k in sorted(counts):\n",
    "    print(f\" - {k:<22} {counts[k]:>12,}\")\n",
    "\n",
    "print(\"\\nOutputs:\")\n",
    "for p in (p_devices, p_texts, p_events, p_dpm, p_codes):\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec6891-3780-4085-829c-94238c038628",
   "metadata": {},
   "source": [
    "DuckDB allows the query of huge CSV/Parquet files with fast SQL, minimal RAM, and no server. It streams data, spills to disk if needed, handles joins across multi-million rows, and exports tidy tables—perfect for a lightweight, reproducible pipeline in a virtualized Ubuntu environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75bfbb3a-69a8-4392-9a38-65f3ab0b539c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckdb 1.4.1\n"
     ]
    }
   ],
   "source": [
    "# === Cell 040: install duckdb ===\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"duckdb\"])\n",
    "import duckdb; print(\"duckdb\", duckdb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3ccee-b98a-4ee3-84c7-9607485818c8",
   "metadata": {},
   "source": [
    "This cell uses DuckDB to join our compact CSVs out-of-core (no giant DataFrames in RAM as this is aVM there were issues in previous attempts). We register devices_min.csv and events_min.csv as views, parse dates, and coalesce event_date (prefer events, else device date_received). We window records to 2021-01-01…2024-12-31 and derive a quarter label (YYYY-Q#). If available, we left-join device-problem codes (and descriptions) from the mapping files. We then export a single analysis table (_analysis_event_level.csv / Parquet) containing event_id, event_date, brand, model, problem_code, problem_desc, and quarter. Finally, we write lightweight diagnostics (quarter totals, missingness by field, top brands, top brand×problem) to validate coverage and guide downstream targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9f8a42-c4a6-4feb-97a6-c61cb33c55c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /home/parallels/data/capstone-maude/Data/outputs/tables/_analysis_event_level.csv\n",
      "Diagnostics saved:\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/quarter_totals.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/quarter_missingness.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/brand_top20.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/brand_problem_top20.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/_analysis_event_level_sample10k.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell 04  Build unified event-level + diagnostics, RAM-safe ===\n",
    "# - Joins devices_min.csv + events_min.csv in DuckDB (out-of-core)\n",
    "# - Coalesces event_date (prefer events.event_date, else devices.date_received)\n",
    "# - Filters to 2021-01-01..2024-12-31\n",
    "# - Computes quarter = 'YYYY-Q#'\n",
    "# - Skips narratives here (huge) to avoid kernel OOM; sample later if needed\n",
    "# - Writes:\n",
    "#     _analysis_event_level.csv  (event_id, event_date, brand, model, problem_code, problem_desc, quarter)\n",
    "#     quarter_totals.csv\n",
    "#     quarter_missingness.csv\n",
    "#     brand_top20.csv\n",
    "#     brand_problem_top20.csv\n",
    "#     _analysis_event_level_sample10k.csv\n",
    "#\n",
    "# NOTE: In this variant, problem_code is NULL (no foidevproblem join). \n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "\n",
    "# ---------- Paths ----------\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\", \"\")).expanduser()\n",
    "assert RAW_DIR.exists(), f\"MAUDE_RAW not set or path missing: {RAW_DIR}\"\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_TAB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "p_devices = OUT_TAB / \"devices_min.csv\"\n",
    "p_events  = OUT_TAB / \"events_min.csv\"\n",
    "p_codes   = OUT_TAB / \"deviceproblemcodes_min.csv\"  # optional dict (used only if present)\n",
    "\n",
    "assert p_devices.exists(), f\"Missing {p_devices}\"\n",
    "assert p_events.exists(),  f\"Missing {p_events}\"\n",
    "\n",
    "# ---------- DB connect ----------\n",
    "db_path = OUT_ROOT / \"maude.duckdb\"\n",
    "con = duckdb.connect(database=str(db_path), read_only=False)\n",
    "\n",
    "# VM-friendly pragmas (tune threads/memory to your VM limits)\n",
    "con.execute(\"PRAGMA threads=4;\")\n",
    "con.execute(\"PRAGMA memory_limit='2GB';\")   # e.g., '2GB' or less if RAM-constrained\n",
    "con.execute(\"PRAGMA preserve_insertion_order=false;\")\n",
    "\n",
    "# ---------- Register CSVs as views ----------\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW devices AS\n",
    "    SELECT\n",
    "        TRIM(CAST(event_id AS VARCHAR)) AS event_id,\n",
    "        NULLIF(TRIM(CAST(brand  AS VARCHAR)), '') AS brand,\n",
    "        NULLIF(TRIM(CAST(model  AS VARCHAR)), '') AS model,\n",
    "        CASE\n",
    "            WHEN date_received ~ '^[0-9]{{1,2}}/[0-9]{{1,2}}/[0-9]{{4}}$'\n",
    "            THEN STRPTIME(date_received, '%m/%d/%Y')\n",
    "            ELSE TRY_STRPTIME(date_received, '%Y-%m-%d')\n",
    "        END AS date_received\n",
    "    FROM read_csv_auto('{p_devices.as_posix()}', header=true, all_varchar=true);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW events AS\n",
    "    SELECT\n",
    "        TRIM(CAST(event_id AS VARCHAR)) AS event_id,\n",
    "        CASE\n",
    "            WHEN event_date ~ '^[0-9]{{1,2}}/[0-9]{{1,2}}/[0-9]{{4}}$'\n",
    "            THEN STRPTIME(event_date, '%m/%d/%Y')\n",
    "            ELSE TRY_STRPTIME(event_date, '%Y-%m-%d')\n",
    "        END AS event_date\n",
    "    FROM read_csv_auto('{p_events.as_posix()}', header=true, all_varchar=true);\n",
    "\"\"\")\n",
    "\n",
    "codes_exists = p_codes.exists() and p_codes.stat().st_size > 0\n",
    "if codes_exists:\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW codes AS\n",
    "        SELECT\n",
    "            TRIM(CAST(FDA_CODE AS VARCHAR)) AS problem_code,\n",
    "            NULLIF(TRIM(CAST(TERM AS VARCHAR)), '') AS problem_desc\n",
    "        FROM read_csv_auto('{p_codes.as_posix()}', header=true, all_varchar=true);\n",
    "    \"\"\")\n",
    "\n",
    "# ---------- Unified view (no foidevproblem join here; problem_code remains NULL) ----------\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW unified AS\n",
    "    SELECT\n",
    "        d.event_id,\n",
    "        COALESCE(e.event_date, d.date_received) AS event_date,\n",
    "        d.brand,\n",
    "        d.model,\n",
    "        CAST(NULL AS VARCHAR) AS problem_code\n",
    "    FROM devices d\n",
    "    LEFT JOIN events e USING (event_id)\n",
    "    WHERE COALESCE(e.event_date, d.date_received)\n",
    "          BETWEEN DATE '2021-01-01' AND DATE '2024-12-31';\n",
    "\"\"\")\n",
    "\n",
    "# ---------- Add quarter & optional problem_desc ----------\n",
    "if codes_exists:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW unified_q AS\n",
    "        SELECT\n",
    "            u.*,\n",
    "            strftime(u.event_date, '%Y') || '-Q' ||\n",
    "              CAST(((CAST(strftime(u.event_date, '%m') AS INTEGER)-1)/3 + 1) AS VARCHAR) AS quarter,\n",
    "            c.problem_desc\n",
    "        FROM unified u\n",
    "        LEFT JOIN codes c USING (problem_code);\n",
    "    \"\"\")\n",
    "else:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW unified_q AS\n",
    "        SELECT\n",
    "            u.*,\n",
    "            strftime(u.event_date, '%Y') || '-Q' ||\n",
    "              CAST(((CAST(strftime(u.event_date, '%m') AS INTEGER)-1)/3 + 1) AS VARCHAR) AS quarter,\n",
    "            CAST(NULL AS VARCHAR) AS problem_desc\n",
    "        FROM unified u;\n",
    "    \"\"\")\n",
    "\n",
    "# ---------- Write unified CSV ----------\n",
    "out_main = OUT_TAB / \"_analysis_event_level.csv\"\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT event_id,\n",
    "               strftime(event_date, '%m/%d/%Y') AS event_date,\n",
    "               brand, model, problem_code, problem_desc, quarter\n",
    "        FROM unified_q\n",
    "        ORDER BY event_date, event_id\n",
    "    ) TO '{out_main.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "print(f\"Wrote {out_main}\")\n",
    "\n",
    "# ---------- Diagnostics ----------\n",
    "out_qtot  = OUT_TAB / \"quarter_totals.csv\"\n",
    "out_qmiss = OUT_TAB / \"quarter_missingness.csv\"\n",
    "out_btop  = OUT_TAB / \"brand_top20.csv\"\n",
    "out_bptop = OUT_TAB / \"brand_problem_top20.csv\"\n",
    "\n",
    "# Quarter totals\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT quarter, COUNT(*) AS total\n",
    "        FROM unified_q\n",
    "        GROUP BY quarter\n",
    "        ORDER BY quarter\n",
    "    ) TO '{out_qtot.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "# Quarter missingness (use SUM(CASE)/COUNT to avoid AVG(boolean))\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT\n",
    "            quarter,\n",
    "            CAST(SUM(CASE WHEN brand IS NULL        THEN 1 ELSE 0 END) AS DOUBLE) / COUNT(*) AS brand_missing,\n",
    "            CAST(SUM(CASE WHEN model IS NULL        THEN 1 ELSE 0 END) AS DOUBLE) / COUNT(*) AS model_missing,\n",
    "            CAST(SUM(CASE WHEN problem_code IS NULL THEN 1 ELSE 0 END) AS DOUBLE) / COUNT(*) AS prob_missing\n",
    "        FROM unified_q\n",
    "        GROUP BY quarter\n",
    "        ORDER BY quarter\n",
    "    ) TO '{out_qmiss.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "# Top brands\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT brand, COUNT(*) AS n\n",
    "        FROM unified_q\n",
    "        GROUP BY brand\n",
    "        ORDER BY n DESC NULLS LAST\n",
    "        LIMIT 20\n",
    "    ) TO '{out_btop.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "# Top brand × problem (problem_code will be NULL here; kept for schema consistency)\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT brand, problem_code, COUNT(*) AS n\n",
    "        FROM unified_q\n",
    "        GROUP BY brand, problem_code\n",
    "        ORDER BY n DESC NULLS LAST\n",
    "        LIMIT 20\n",
    "    ) TO '{out_bptop.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "# Optional: 10k sample for quick/local checks\n",
    "out_samp = OUT_TAB / \"_analysis_event_level_sample10k.csv\"\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM unified_q\n",
    "        USING SAMPLE 10000 ROWS\n",
    "    ) TO '{out_samp.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "print(\"Diagnostics saved:\")\n",
    "print(\" -\", out_qtot)\n",
    "print(\" -\", out_qmiss)\n",
    "print(\" -\", out_btop)\n",
    "print(\" -\", out_bptop)\n",
    "print(\" -\", out_samp)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5a519f-109e-4f60-969e-644f7069e9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built foidevproblem_min.csv → 22,186,060 rows at /home/parallels/data/capstone-maude/Data/outputs/tables/foidevproblem_min.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell 04A: Build foidevproblem_min.csv (event_id, problem_code) ===\n",
    "# Handles headerless foidevproblem.txt where the first row is numeric data.\n",
    "import os, io, zipfile, re, csv\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_TAB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "zip_path = RAW_DIR / \"foidevproblem.zip\"\n",
    "assert zip_path.exists(), f\"Missing {zip_path}\"\n",
    "\n",
    "out_csv = OUT_TAB / \"foidevproblem_min.csv\"\n",
    "# out_csv.unlink(missing_ok=True)  # uncomment to rebuild from scratch\n",
    "\n",
    "def looks_like_header(fields):\n",
    "    \"\"\"Return True if fields look like a text header (any alphabetic chars).\"\"\"\n",
    "    for f in fields:\n",
    "        if re.search(r\"[A-Za-z]\", f or \"\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "    # pick member\n",
    "    names = [n for n in z.namelist() if not n.lower().endswith((\"/\",\"readme\",\"readme.txt\"))]\n",
    "    member = None\n",
    "    for cand in [\"foidevproblem.txt\", \"FOIDEVPROBLEM.txt\"]:\n",
    "        if cand in names:\n",
    "            member = cand\n",
    "            break\n",
    "    if member is None:\n",
    "        member = max(names, key=lambda n: z.getinfo(n).file_size)\n",
    "\n",
    "    # try encodings\n",
    "    encodings = [\"utf-8\", \"latin-1\"]\n",
    "    picked_enc = None\n",
    "    delim = \"|\"\n",
    "\n",
    "    # peek first line to detect delimiter + headerish\n",
    "    header_fields = None\n",
    "    header_is_real = False\n",
    "    for enc in encodings:\n",
    "        with z.open(member, \"r\") as fb:\n",
    "            text = io.TextIOWrapper(fb, encoding=enc, errors=\"ignore\", newline=\"\")\n",
    "            first = text.readline().rstrip(\"\\r\\n\")\n",
    "            if not first:\n",
    "                continue\n",
    "            # delimiter guess\n",
    "            delim = \"|\" if first.count(\"|\") >= first.count(\",\") else \",\"\n",
    "            header_fields = [c.strip() for c in first.split(delim)]\n",
    "            picked_enc = enc\n",
    "            header_is_real = looks_like_header(header_fields)\n",
    "            break\n",
    "    if picked_enc is None:\n",
    "        raise RuntimeError(\"Could not read first line of foidevproblem with tried encodings.\")\n",
    "\n",
    "    # Decide column indices\n",
    "    if header_is_real:\n",
    "        # find indices by normalized names\n",
    "        def norm(s): return re.sub(r\"[^A-Z0-9]\", \"\", s.upper())\n",
    "        names_norm = [norm(c) for c in header_fields]\n",
    "        # candidates\n",
    "        evt_cands = {\"MDRREPORTKEY\",\"REPORTKEY\",\"MDRREPORTID\",\"MDRREPKEY\"}\n",
    "        code_cands = {\"DEVICEPROBLEMCODE\",\"PROBLEMCODE\",\"DVCPROBLEMCODE\"}\n",
    "        try:\n",
    "            idx_event = next(i for i, n in enumerate(names_norm) if n in evt_cands)\n",
    "            idx_code  = next(i for i, n in enumerate(names_norm) if n in code_cands)\n",
    "        except StopIteration:\n",
    "            # fallback to first two columns if header weird but present\n",
    "            idx_event, idx_code = 0, 1\n",
    "    else:\n",
    "        # headerless numeric first row → assume two-column layout\n",
    "        idx_event, idx_code = 0, 1\n",
    "\n",
    "    # stream rows\n",
    "    n = 0\n",
    "    with z.open(member, \"r\") as fb, open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "        text = io.TextIOWrapper(fb, encoding=picked_enc, errors=\"ignore\", newline=\"\")\n",
    "        w = csv.writer(fout)\n",
    "        w.writerow([\"event_id\",\"problem_code\"])\n",
    "\n",
    "        first_line = text.readline()  # already read; skip if header, else process\n",
    "        if not header_is_real:\n",
    "            # first line is data\n",
    "            row = [c.strip() for c in first_line.rstrip(\"\\r\\n\").split(delim)]\n",
    "            if len(row) > max(idx_event, idx_code):\n",
    "                ev, pc = row[idx_event], row[idx_code]\n",
    "                if ev and pc:\n",
    "                    w.writerow([ev, pc])\n",
    "                    n += 1\n",
    "\n",
    "        for raw in text:\n",
    "            row = [c.strip() for c in raw.rstrip(\"\\r\\n\").split(delim)]\n",
    "            if len(row) <= max(idx_event, idx_code):\n",
    "                continue\n",
    "            ev, pc = row[idx_event], row[idx_code]\n",
    "            if not ev or not pc:\n",
    "                continue\n",
    "            w.writerow([ev, pc])\n",
    "            n += 1\n",
    "\n",
    "print(f\"Built {out_csv.name} → {n:,} rows at {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256e726-2158-4b0d-ab0d-665c7679b907",
   "metadata": {},
   "source": [
    "Cell 04B refreshes the unified, analysis-ready table after we successfully extract the foidevproblem_min.csv mapping. Instead of re-running the heavy ingestion (Cell 03) or rebuilding all diagnostics, this patch step joins problem codes (and descriptions, if available) onto the already normalized event/device core, recomputes quarters, and rewrites _analysis_event_level.csv. Downstream analytics—PRR/ROR screening (Cell 06), ITS/plots (Cell 07), and pre/post summaries (Cell 08)—all depend on having an accurate problem_code column at event grain. Implemented in DuckDB, it’s fast, idempotent, and RAM-safe, preserving earlier diagnostics while ensuring the canonical dataset contains the fields required for statistical testing and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03cbe88-7e8e-4f6b-820d-fa6e50175dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built /home/parallels/data/capstone-maude/Data/outputs/tables/foidevproblem_min.csv → 22,186,060 rows (enc=utf-8, sep='|', header=no)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4b: Build foidevproblem_min.csv (streaming, robust) ===\n",
    "from pathlib import Path\n",
    "import os, zipfile, csv, io, re\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_TAB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "choices_path = OUT_TAB / \"chosen_members.csv\"\n",
    "assert choices_path.exists(), f\"Missing {choices_path} (run Cell 02).\"\n",
    "choices = pd.read_csv(choices_path)\n",
    "\n",
    "row = choices.loc[choices[\"zip\"] == \"foidevproblem.zip\"]\n",
    "assert not row.empty, \"foidevproblem.zip not found in chosen_members.csv.\"\n",
    "member = row.iloc[0][\"chosen_member\"]\n",
    "sep    = row.iloc[0][\"guessed_delimiter\"] or \"|\"\n",
    "\n",
    "zpath  = RAW_DIR / \"foidevproblem.zip\"\n",
    "assert zpath.exists(), f\"Missing {zpath}\"\n",
    "\n",
    "out_csv = OUT_TAB / \"foidevproblem_min.csv\"\n",
    "if out_csv.exists():\n",
    "    out_csv.unlink()\n",
    "\n",
    "# allow long fields\n",
    "csv.field_size_limit(min(2_000_000_000, sys.maxsize))\n",
    "\n",
    "ENCODINGS = (\"utf-8\", \"latin-1\")\n",
    "CHUNK = 200_000  # tune smaller if memory is tight\n",
    "\n",
    "def _peek_header(zf, member, enc):\n",
    "    \"\"\"Return first line as list of tokens; also return raw line for checks.\"\"\"\n",
    "    with zf.open(member, \"r\") as fb:\n",
    "        text = io.TextIOWrapper(fb, encoding=enc, errors=\"ignore\", newline=\"\")\n",
    "        line = text.readline()\n",
    "    return line.strip(\"\\r\\n\"), line\n",
    "\n",
    "with zipfile.ZipFile(zpath) as zf:\n",
    "    # Try encodings until we can inspect the first line reliably\n",
    "    used_enc = None\n",
    "    header_tokens = None\n",
    "    raw_line = \"\"\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            first, raw_line = _peek_header(zf, member, enc)\n",
    "            header_tokens = first.split(sep)\n",
    "            used_enc = enc\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    assert used_enc is not None, \"Failed to read foidevproblem member with utf-8/latin-1.\"\n",
    "\n",
    "    # Heuristic: if the first two tokens look like numbers, assume NO HEADER\n",
    "    def looks_numeric(s):\n",
    "        s = (s or \"\").strip()\n",
    "        return bool(re.fullmatch(r\"[0-9]+\", s))\n",
    "\n",
    "    has_header = True\n",
    "    if len(header_tokens) >= 2 and looks_numeric(header_tokens[0]) and looks_numeric(header_tokens[1]):\n",
    "        has_header = False\n",
    "\n",
    "    # Column resolution:\n",
    "    # - If header present, try regex name match; else fallback to positional names.\n",
    "    name_map = {\"event_id\": None, \"problem_code\": None}\n",
    "    if has_header:\n",
    "        cols_lower = [c.strip().lower() for c in header_tokens]\n",
    "        def find_col(patterns):\n",
    "            for i, cname in enumerate(cols_lower):\n",
    "                for pat in patterns:\n",
    "                    if re.search(pat, cname, re.I):\n",
    "                        return i\n",
    "            return None\n",
    "        idx_event = find_col([r\"^mdr[_ ]?report[_ ]?key$\", r\"^report[_ ]?key$\"])\n",
    "        idx_code  = find_col([r\"^device[_ ]?problem[_ ]?code$\", r\"^problem[_ ]?code$\"])\n",
    "        # If not found, fallback to first two columns\n",
    "        if idx_event is None: idx_event = 0\n",
    "        if idx_code  is None: idx_code  = 1\n",
    "    else:\n",
    "        idx_event, idx_code = 0, 1\n",
    "\n",
    "    # Stream with pandas; when header is missing, use header=None + names\n",
    "    n_written = 0\n",
    "    wrote_header = False\n",
    "\n",
    "    with zf.open(member, \"r\") as fb:\n",
    "        text = io.TextIOWrapper(fb, encoding=used_enc, errors=\"ignore\", newline=\"\")\n",
    "        # If header exists, let pandas parse it; else supply names\n",
    "        reader = pd.read_csv(\n",
    "            text,\n",
    "            sep=sep,\n",
    "            engine=\"python\",\n",
    "            dtype=str,\n",
    "            on_bad_lines=\"skip\",\n",
    "            quoting=csv.QUOTE_NONE if sep == \"|\" else csv.QUOTE_MINIMAL,\n",
    "            escapechar=\"\\\\\" if sep == \"|\" else None,\n",
    "            chunksize=CHUNK,\n",
    "            header=0 if has_header else None\n",
    "        )\n",
    "\n",
    "        # If no header: build a names list of correct width on first chunk\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if not has_header and i == 0:\n",
    "                # ensure we know width\n",
    "                width = len(chunk.columns)\n",
    "                names = [f\"col{j}\" for j in range(width)]\n",
    "                chunk.columns = names\n",
    "            # Select columns by position\n",
    "            cols = list(chunk.columns)\n",
    "            try:\n",
    "                ev_col = cols[idx_event]\n",
    "                pc_col = cols[idx_code]\n",
    "            except Exception:\n",
    "                # fallback to first two\n",
    "                ev_col, pc_col = cols[0], cols[1]\n",
    "\n",
    "            sub = chunk[[ev_col, pc_col]].copy()\n",
    "            sub.columns = [\"event_id\", \"problem_code\"]\n",
    "            # strip and drop empties\n",
    "            sub[\"event_id\"] = sub[\"event_id\"].astype(str).str.strip()\n",
    "            sub[\"problem_code\"] = sub[\"problem_code\"].astype(str).str.strip()\n",
    "            sub = sub[(sub[\"event_id\"] != \"\") & (sub[\"problem_code\"] != \"\")]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            sub.to_csv(out_csv, index=False, mode=\"w\" if not wrote_header else \"a\",\n",
    "                       header=not wrote_header)\n",
    "            wrote_header = True\n",
    "            n_written += len(sub)\n",
    "            if n_written and n_written % 1_000_000 == 0:\n",
    "                print(f\"... wrote {n_written:,} rows\")\n",
    "\n",
    "print(f\"Built {out_csv} → {n_written:,} rows (enc={used_enc}, sep='{sep}', header={'yes' if has_header else 'no'})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d106a-36ae-4112-8487-bed1b7438f81",
   "metadata": {},
   "source": [
    "This cell refreshes the master analysis table after I've created foidevproblem_min.csv. Using DuckDB (out-of-core), it re-reads the compact CSVs (devices_min, events_min, foidevproblem_min, and optionally deviceproblemcodes_min) and left-joins problem codes—plus human-readable descriptions when available—onto the event grain. Dates are parsed consistently, event_date is coalesced, and quarter (YYYY-Q#) is derived. We then export _analysis_event_level.csv (and/or Parquet) without expensive in-memory sorts to keep memory usage low in the VM. Use this when you add or fix the device-problem mapping and want the unified dataset to reflect those codes without rerunning the earlier heavy extraction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26de5c0-4885-4a3e-b309-3eb62019b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote _analysis_event_level.parquet and _analysis_event_level.csv | rows=11,693,760\n",
      "Diagnostics saved:\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/quarter_totals.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/quarter_missingness.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/brand_top20.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/brand_problem_top20.csv\n",
      " - /home/parallels/data/capstone-maude/Data/outputs/tables/_analysis_event_level_sample10k.csv\n",
      "✓ Done (no ORDER BY to keep RAM low).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 04d: Rebuild unified CSV with problem_code join (RAM-safe, in DuckDB) ===\n",
    "# Prereqs:\n",
    "#   • Cell 03 created: devices_min.csv, events_min.csv, deviceproblemcodes_min.csv\n",
    "#   • Cell 04C created: foidevproblem_min.csv   <-- you just ran this\n",
    "# Outputs (under Data/outputs/tables):\n",
    "#   • _analysis_event_level.csv  (event_id, event_date, brand, model, problem_code, problem_desc, quarter)\n",
    "#   • quarter_totals.csv\n",
    "#   • quarter_missingness.csv\n",
    "#   • brand_top20.csv\n",
    "#   • brand_problem_top20.csv\n",
    "#   • _analysis_event_level_sample10k.csv\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import duckdb\n",
    "\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "assert RAW_DIR.exists(), f\"MAUDE_RAW not set or path missing: {RAW_DIR}\"\n",
    "\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_FIG  = OUT_ROOT / \"figures\"\n",
    "for p in (OUT_ROOT, OUT_TAB, OUT_FIG):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "p_devices = OUT_TAB / \"devices_min.csv\"\n",
    "p_events  = OUT_TAB / \"events_min.csv\"\n",
    "p_dpm     = OUT_TAB / \"foidevproblem_min.csv\"        # from Cell 04C\n",
    "p_codes   = OUT_TAB / \"deviceproblemcodes_min.csv\"   # optional, adds problem_desc\n",
    "\n",
    "assert p_devices.exists(), f\"Missing {p_devices}\"\n",
    "assert p_events.exists(),  f\"Missing {p_events}\"\n",
    "assert p_dpm.exists() and p_dpm.stat().st_size > 0, \"foidevproblem_min.csv not found or empty. Run Cell 04C first.\"\n",
    "\n",
    "db_path = OUT_ROOT / \"maude.duckdb\"\n",
    "tmp_dir = OUT_ROOT / \"duckdb_tmp\"\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con = duckdb.connect(database=str(db_path), read_only=False)\n",
    "\n",
    "# Constrain resources (VM-friendly)\n",
    "con.execute(\"PRAGMA threads=4;\")\n",
    "con.execute(\"PRAGMA memory_limit='3GB';\")\n",
    "con.execute(f\"PRAGMA temp_directory='{tmp_dir.as_posix()}';\")\n",
    "\n",
    "# Views over CSVs (all_varchar to avoid type surprises; we cast/parse explicitly)\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW devices AS\n",
    "    SELECT\n",
    "        TRIM(CAST(event_id AS VARCHAR)) AS event_id,\n",
    "        NULLIF(TRIM(CAST(brand  AS VARCHAR)), '') AS brand,\n",
    "        NULLIF(TRIM(CAST(model  AS VARCHAR)), '') AS model,\n",
    "        CASE\n",
    "            WHEN date_received ~ '^[0-9]{{1,2}}/[0-9]{{1,2}}/[0-9]{{4}}$'\n",
    "            THEN STRPTIME(date_received, '%m/%d/%Y')\n",
    "            ELSE TRY_STRPTIME(date_received, '%Y-%m-%d')\n",
    "        END AS date_received\n",
    "    FROM read_csv_auto('{p_devices.as_posix()}', header=true, all_varchar=true);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW events AS\n",
    "    SELECT\n",
    "        TRIM(CAST(event_id AS VARCHAR)) AS event_id,\n",
    "        CASE\n",
    "            WHEN event_date ~ '^[0-9]{{1,2}}/[0-9]{{1,2}}/[0-9]{{4}}$'\n",
    "            THEN STRPTIME(event_date, '%m/%d/%Y')\n",
    "            ELSE TRY_STRPTIME(event_date, '%Y-%m-%d')\n",
    "        END AS event_date\n",
    "    FROM read_csv_auto('{p_events.as_posix()}', header=true, all_varchar=true);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW devprob AS\n",
    "    SELECT\n",
    "        TRIM(CAST(event_id     AS VARCHAR)) AS event_id,\n",
    "        NULLIF(TRIM(CAST(problem_code AS VARCHAR)), '') AS problem_code\n",
    "    FROM read_csv_auto('{p_dpm.as_posix()}', header=true, all_varchar=true);\n",
    "\"\"\")\n",
    "\n",
    "codes_exists = p_codes.exists() and p_codes.stat().st_size > 0\n",
    "if codes_exists:\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW codes AS\n",
    "        SELECT\n",
    "            TRIM(CAST(FDA_CODE AS VARCHAR)) AS problem_code,\n",
    "            NULLIF(TRIM(CAST(TERM     AS VARCHAR)), '') AS problem_desc\n",
    "        FROM read_csv_auto('{p_codes.as_posix()}', header=true, all_varchar=true);\n",
    "    \"\"\")\n",
    "\n",
    "# Unified (no ORDER BY here → avoids large memory spikes)\n",
    "if codes_exists:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW unified_q AS\n",
    "        SELECT\n",
    "            d.event_id,\n",
    "            COALESCE(e.event_date, d.date_received) AS event_date,\n",
    "            d.brand,\n",
    "            d.model,\n",
    "            dp.problem_code,\n",
    "            c.problem_desc,\n",
    "            strftime(COALESCE(e.event_date, d.date_received), '%Y') || '-Q' ||\n",
    "              CAST(((CAST(strftime(COALESCE(e.event_date, d.date_received), '%m') AS INTEGER)-1)/3 + 1) AS VARCHAR) AS quarter\n",
    "        FROM devices d\n",
    "        LEFT JOIN events  e  USING (event_id)\n",
    "        LEFT JOIN devprob dp USING (event_id)\n",
    "        LEFT JOIN codes   c  USING (problem_code)\n",
    "        WHERE COALESCE(e.event_date, d.date_received)\n",
    "              BETWEEN DATE '2021-01-01' AND DATE '2024-12-31';\n",
    "    \"\"\")\n",
    "else:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW unified_q AS\n",
    "        SELECT\n",
    "            d.event_id,\n",
    "            COALESCE(e.event_date, d.date_received) AS event_date,\n",
    "            d.brand,\n",
    "            d.model,\n",
    "            dp.problem_code,\n",
    "            CAST(NULL AS VARCHAR) AS problem_desc,\n",
    "            strftime(COALESCE(e.event_date, d.date_received), '%Y') || '-Q' ||\n",
    "              CAST(((CAST(strftime(COALESCE(e.event_date, d.date_received), '%m') AS INTEGER)-1)/3 + 1) AS VARCHAR) AS quarter\n",
    "        FROM devices d\n",
    "        LEFT JOIN events  e  USING (event_id)\n",
    "        LEFT JOIN devprob dp USING (event_id)\n",
    "        WHERE COALESCE(e.event_date, d.date_received)\n",
    "              BETWEEN DATE '2021-01-01' AND DATE '2024-12-31';\n",
    "    \"\"\")\n",
    "\n",
    "# Write Parquet first (fast/stable), then CSV (full), plus 10k sample\n",
    "out_parq  = OUT_TAB / \"_analysis_event_level.parquet\"\n",
    "out_csv   = OUT_TAB / \"_analysis_event_level.csv\"\n",
    "out_samp  = OUT_TAB / \"_analysis_event_level_sample10k.csv\"\n",
    "\n",
    "con.execute(f\"COPY unified_q TO '{out_parq.as_posix()}' (FORMAT PARQUET);\")\n",
    "con.execute(f\"COPY unified_q TO '{out_csv.as_posix()}'  WITH (HEADER, DELIMITER ',');\")\n",
    "con.execute(f\"COPY (SELECT * FROM unified_q LIMIT 10000) TO '{out_samp.as_posix()}' WITH (HEADER, DELIMITER ',');\")\n",
    "\n",
    "# Diagnostics (lightweight)\n",
    "out_qtot  = OUT_TAB / \"quarter_totals.csv\"\n",
    "out_qmiss = OUT_TAB / \"quarter_missingness.csv\"\n",
    "out_btop  = OUT_TAB / \"brand_top20.csv\"\n",
    "out_bptop = OUT_TAB / \"brand_problem_top20.csv\"\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT quarter, COUNT(*) AS total\n",
    "        FROM unified_q\n",
    "        GROUP BY quarter\n",
    "        ORDER BY quarter\n",
    "    ) TO '{out_qtot.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT\n",
    "            quarter,\n",
    "            CAST(SUM(CASE WHEN brand        IS NULL THEN 1 ELSE 0 END) AS DOUBLE) / COUNT(*) AS brand_missing,\n",
    "            CAST(SUM(CASE WHEN model        IS NULL THEN 1 ELSE 0 END) AS DOUBLE) / COUNT(*) AS model_missing,\n",
    "            CAST(SUM(CASE WHEN problem_code IS NULL THEN 1 ELSE 0 END) AS DOUBLE) / COUNT(*) AS prob_missing\n",
    "        FROM unified_q\n",
    "        GROUP BY quarter\n",
    "        ORDER BY quarter\n",
    "    ) TO '{out_qmiss.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT brand, COUNT(*) AS n\n",
    "        FROM unified_q\n",
    "        GROUP BY brand\n",
    "        ORDER BY n DESC NULLS LAST\n",
    "        LIMIT 20\n",
    "    ) TO '{out_btop.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT brand, problem_code, COUNT(*) AS n\n",
    "        FROM unified_q\n",
    "        GROUP BY brand, problem_code\n",
    "        ORDER BY n DESC NULLS LAST\n",
    "        LIMIT 20\n",
    "    ) TO '{out_bptop.as_posix()}' WITH (HEADER, DELIMITER ',');\n",
    "\"\"\")\n",
    "\n",
    "n_rows = con.execute(\"SELECT COUNT(*) FROM unified_q;\").fetchone()[0]\n",
    "con.close()\n",
    "\n",
    "print(f\"Wrote {out_parq.name} and {out_csv.name} | rows={n_rows:,}\")\n",
    "print(\"Diagnostics saved:\")\n",
    "print(\" -\", out_qtot)\n",
    "print(\" -\", out_qmiss)\n",
    "print(\" -\", out_btop)\n",
    "print(\" -\", out_bptop)\n",
    "print(\" -\", out_samp)\n",
    "print(\"✓ Done (no ORDER BY to keep RAM low).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6556c7b-8e62-4a74-990c-63fae99b6b9b",
   "metadata": {},
   "source": [
    "This cell sanity-checks the unified dataset and picks concrete brand×problem pairs for downstream tests and slides. First, it loads _analysis_event_level.csv and computes simple quality metrics (row count and missingness rates for brand, model, and problem_code). Next, it profiles brand frequency, writes tidy CSV summaries, and renders a “Top 12 Brands (2021–2024)” figure for quick context. Finally, it auto-selects two analysis targets by taking the most frequent brand×problem combinations with non-null codes, saving them to targets_selected.csv. These targets flow into disproportionality (PRR/ROR) and ITS visualizations, ensuring the remainder of the notebook focuses on signal-rich, reproducible examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2033c1cc-6120-4549-b7fb-cbd1a5c9df66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...processed ~5,000,000 rows\n",
      "...processed ~10,000,000 rows\n",
      "Loaded df (streamed): 11,693,760 rows | cols=['event_id', 'brand', 'problem_code', 'quarter']\n",
      "\n",
      "Auto-selected targets (brand × problem):\n",
      "- DEXCOM G6 CONTINUOUS GLUCOSE MONITORING SYSTEM × 3283 (n=430,961)\n",
      "- ALARIS PUMP MODULE × 1135 (n=641,340)\n",
      "\n",
      "Saved:\n",
      "- /home/parallels/data/capstone-maude/Data/outputs/tables/missingness_summary.csv\n",
      "- /home/parallels/data/capstone-maude/Data/outputs/tables/brand_counts.csv\n",
      "- /home/parallels/data/capstone-maude/Data/outputs/tables/targets_selected.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell 05A: Profiling + Auto-select Targets (chunk-safe) ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- paths ----------\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_FIG  = OUT_ROOT / \"figures\"\n",
    "for p in (OUT_TAB, OUT_FIG): \n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ANALYSIS = OUT_TAB / \"_analysis_event_level.csv\"\n",
    "assert ANALYSIS.exists(), f\"Missing {ANALYSIS}. Run Cell 04B first.\"\n",
    "\n",
    "# ---------- config ----------\n",
    "usecols   = [\"event_id\",\"brand\",\"problem_code\",\"quarter\"]\n",
    "chunksize = 1_000_000\n",
    "\n",
    "# ---------- running tallies ----------\n",
    "total_rows = 0\n",
    "brand_missing = 0\n",
    "problem_missing = 0\n",
    "brand_counts = Counter()\n",
    "pair_counts  = Counter()\n",
    "\n",
    "reader = pd.read_csv(\n",
    "    ANALYSIS,\n",
    "    usecols=usecols,\n",
    "    dtype=str,\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    chunksize=chunksize\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(reader, 1):\n",
    "    chunk[\"brand\"] = chunk[\"brand\"].where(chunk[\"brand\"].notna(), None)\n",
    "    chunk[\"problem_code\"] = chunk[\"problem_code\"].where(chunk[\"problem_code\"].notna(), None)\n",
    "\n",
    "    n = len(chunk)\n",
    "    total_rows += n\n",
    "    brand_missing   += chunk[\"brand\"].isna().sum()\n",
    "    problem_missing += chunk[\"problem_code\"].isna().sum()\n",
    "\n",
    "    brand_counts.update(chunk[\"brand\"].tolist())\n",
    "\n",
    "    nonnull = chunk.dropna(subset=[\"brand\",\"problem_code\"])\n",
    "    if not nonnull.empty:\n",
    "        grp = nonnull.groupby([\"brand\",\"problem_code\"])[\"event_id\"].size()\n",
    "        pair_counts.update({k: int(v) for k, v in grp.items()})\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"...processed ~{total_rows:,} rows\")\n",
    "\n",
    "print(f\"Loaded df (streamed): {total_rows:,} rows | cols={usecols}\")\n",
    "\n",
    "# ---------- missingness summary ----------\n",
    "miss = pd.DataFrame({\n",
    "    \"n_rows\": [total_rows],\n",
    "    \"brand_missing_rate\": [brand_missing / max(total_rows,1)],\n",
    "    \"model_missing_rate\": [np.nan],\n",
    "    \"problem_code_missing_rate\": [problem_missing / max(total_rows,1)],\n",
    "})\n",
    "miss_path = OUT_TAB / \"missingness_summary.csv\"\n",
    "miss.to_csv(miss_path, index=False)\n",
    "\n",
    "# ---------- brand counts (top 12) ----------\n",
    "bc_items = [(b if b is not None else np.nan, c) for b, c in brand_counts.items()]\n",
    "brand_counts_df = (\n",
    "    pd.DataFrame(bc_items, columns=[\"brand\",\"n\"])\n",
    "    .sort_values(\"n\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "brand_counts_path = OUT_TAB / \"brand_counts.csv\"\n",
    "brand_counts_df.to_csv(brand_counts_path, index=False)\n",
    "\n",
    "# ---------- auto-select 2 targets ----------\n",
    "topN = 12\n",
    "top_brands = brand_counts_df.head(topN).copy()\n",
    "top_brands[\"brand\"] = top_brands[\"brand\"].astype(object).where(top_brands[\"brand\"].notna(), \"«missing»\")\n",
    "\n",
    "targets = []\n",
    "for _, row in top_brands.iterrows():\n",
    "    b = row[\"brand\"]\n",
    "    if not isinstance(b, str) or b == \"«missing»\":\n",
    "        continue\n",
    "    best_pc, best_cnt = None, -1\n",
    "    for (bb, pc), cnt in pair_counts.items():\n",
    "        if bb == b and cnt > best_cnt:\n",
    "            best_pc, best_cnt = pc, cnt\n",
    "    if best_pc is not None:\n",
    "        targets.append({\"brand\": b, \"problem_code\": best_pc, \"count\": int(best_cnt)})\n",
    "    if len(targets) == 2:\n",
    "        break\n",
    "\n",
    "if len(targets) < 2 and pair_counts:\n",
    "    for (bb, pc), cnt in sorted(pair_counts.items(), key=lambda kv: kv[1], reverse=True):\n",
    "        if all(not (t[\"brand\"]==bb and t[\"problem_code\"]==pc) for t in targets):\n",
    "            targets.append({\"brand\": bb, \"problem_code\": pc, \"count\": int(cnt)})\n",
    "        if len(targets) >= 2:\n",
    "            break\n",
    "\n",
    "tgt_path = OUT_TAB / \"targets_selected.csv\"\n",
    "pd.DataFrame(targets).to_csv(tgt_path, index=False)\n",
    "\n",
    "print(\"\\nAuto-selected targets (brand × problem):\")\n",
    "for t in targets:\n",
    "    print(f\"- {t['brand']} × {t['problem_code']} (n={t['count']:,})\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"-\", miss_path)\n",
    "print(\"-\", brand_counts_path)\n",
    "print(\"-\", tgt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f0f56-99ad-47b9-a28b-1ab951207d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 05B: Visualization  (Top-N brands) ===\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Load the outputs from Cell 5A\n",
    "brand_counts_df = pd.read_csv(OUT_TAB / \"brand_counts.csv\")\n",
    "topN = 12\n",
    "top_brands = brand_counts_df.head(topN).copy()\n",
    "top_brands[\"brand\"] = top_brands[\"brand\"].astype(object).where(top_brands[\"brand\"].notna(), \"«missing»\")\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "if len(top_brands):\n",
    "    plt.barh(top_brands[\"brand\"].iloc[::-1], top_brands[\"n\"].iloc[::-1])\n",
    "plt.xlabel(\"Event count (2021–2024)\")\n",
    "plt.title(f\"Top {topN} brands by event count (2021–2024)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = OUT_FIG / f\"top{topN}_brands_2021_2024.png\"\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved figure:\", fig_path)\n",
    "display(Image(filename=str(fig_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c54312-6f65-45a6-91bf-6932b454e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 06: PRR/ROR for selected targets (VM-friendly) ===\n",
    "# Inputs:\n",
    "#   - tables/_analysis_event_level.csv (from Cell 04B)\n",
    "#   - tables/targets_selected.csv      (from Cell 05)\n",
    "# Output:\n",
    "#   - tables/prr_ror_results_for_targets_2021_2024.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import os, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- paths ----------\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "ANALYSIS = OUT_TAB / \"_analysis_event_level.csv\"\n",
    "TGT_CSV  = OUT_TAB / \"targets_selected.csv\"\n",
    "OUT_CSV  = OUT_TAB / \"prr_ror_results_for_targets_2021_2024.csv\"\n",
    "\n",
    "assert ANALYSIS.exists(), f\"Missing {ANALYSIS}. Run Cell 04B.\"\n",
    "assert TGT_CSV.exists(),  f\"Missing {TGT_CSV}. Run Cell 05.\"\n",
    "\n",
    "# ---------- load minimal columns ----------\n",
    "usecols = [\"event_id\",\"brand\",\"problem_code\",\"quarter\"]\n",
    "df = pd.read_csv(ANALYSIS, usecols=usecols, dtype=str, engine=\"c\", low_memory=False)\n",
    "\n",
    "# NA-safe helpers\n",
    "df[\"brand_ok\"] = df[\"brand\"].notna()\n",
    "df[\"prob_ok\"]  = df[\"problem_code\"].notna()\n",
    "\n",
    "targets = pd.read_csv(TGT_CSV).to_dict(\"records\")\n",
    "\n",
    "# Try to import exact tests; otherwise we’ll do chi-square\n",
    "try:\n",
    "    from scipy.stats import chi2_contingency, fisher_exact\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "def prr_ci(A,B,C,D):\n",
    "    # PRR = (A/(A+B)) / (C/(C+D))\n",
    "    # log-PRR var ≈ 1/A - 1/(A+B) + 1/C - 1/(C+D)\n",
    "    A,B,C,D = map(float, (A,B,C,D))\n",
    "    # continuity: avoid division by zero\n",
    "    if A==0 or C==0 or (A+B)==0 or (C+D)==0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    prr = (A/(A+B)) / (C/(C+D))\n",
    "    var = (1/max(A,1.0)) - (1/max(A+B,1.0)) + (1/max(C,1.0)) - (1/max(C+D,1.0))\n",
    "    se  = math.sqrt(max(var, 0.0))\n",
    "    lcl = math.exp(math.log(prr) - 1.96*se)\n",
    "    ucl = math.exp(math.log(prr) + 1.96*se)\n",
    "    return (prr, lcl, ucl)\n",
    "\n",
    "def ror_ci(A,B,C,D):\n",
    "    # ROR = (A/B)/(C/D) = AD/BC\n",
    "    A,B,C,D = map(float, (A,B,C,D))\n",
    "    if A==0 or B==0 or C==0 or D==0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    ror = (A*D)/(B*C)\n",
    "    var = (1/A) + (1/B) + (1/C) + (1/D)\n",
    "    se  = math.sqrt(max(var, 0.0))\n",
    "    lcl = math.exp(math.log(ror) - 1.96*se)\n",
    "    ucl = math.exp(math.log(ror) + 1.96*se)\n",
    "    return (ror, lcl, ucl)\n",
    "\n",
    "def test_2x2(A,B,C,D):\n",
    "    table = np.array([[A,B],[C,D]], dtype=float)\n",
    "    test_name, stat, p = \"Chi-square\", np.nan, np.nan\n",
    "    if HAVE_SCIPY:\n",
    "        # prefer Fisher when any expected <5; else chi-square\n",
    "        expected = (table.sum(axis=1).reshape(2,1) * table.sum(axis=0).reshape(1,2)) / table.sum()\n",
    "        if (expected < 5).any():\n",
    "            test_name = \"Fisher exact\"\n",
    "            try:\n",
    "                _, p = fisher_exact(table, alternative=\"two-sided\")\n",
    "            except Exception:\n",
    "                test_name = \"Chi-square\"\n",
    "                stat, p, _, _ = chi2_contingency(table, correction=True)\n",
    "        else:\n",
    "            test_name = \"Chi-square\"\n",
    "            stat, p, _, _ = chi2_contingency(table, correction=False)\n",
    "    else:\n",
    "        # simple chi-square approximation (Yates correction if any cell < 5)\n",
    "        correction = (A<5 or B<5 or C<5 or D<5)\n",
    "        if not correction and (A+B+C+D) > 0:\n",
    "            # no-correction contingency\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                stat, p = chi2_contingency(table, correction=False)[0:2]\n",
    "            test_name = \"Chi-square\"\n",
    "        else:\n",
    "            # fall back to correction=True if SciPy available; otherwise p remains nan\n",
    "            test_name = \"Chi-square (approx)\"\n",
    "    return test_name, stat, p\n",
    "\n",
    "rows = []\n",
    "for t in targets:\n",
    "    b = str(t[\"brand\"])\n",
    "    pc = str(t[\"problem_code\"])\n",
    "    # counts\n",
    "    mask_brand   = df[\"brand\"].eq(b)\n",
    "    mask_prob    = df[\"problem_code\"].eq(pc)\n",
    "    mask_both    = mask_brand & mask_prob\n",
    "    mask_not_b   = ~mask_brand\n",
    "    mask_not_pc  = ~mask_prob\n",
    "\n",
    "    A = int(mask_both.sum())                 # Brand b & problem pc\n",
    "    B = int((mask_brand & mask_not_pc).sum())# Brand b & other problems\n",
    "    C = int((mask_not_b & mask_prob).sum())  # Other brands & problem pc\n",
    "    D = int((mask_not_b & mask_not_pc).sum())# Other brands & other problems\n",
    "\n",
    "    prr, prr_l, prr_u = prr_ci(A,B,C,D)\n",
    "    ror, ror_l, ror_u = ror_ci(A,B,C,D)\n",
    "    test, stat, p = test_2x2(A,B,C,D)\n",
    "\n",
    "    rows.append({\n",
    "        \"brand\": b, \"problem_code\": pc,\n",
    "        \"A_brand&problem\": A, \"B_brand&other\": B, \"C_other&problem\": C, \"D_other&other\": D,\n",
    "        \"PRR\": prr, \"PRR_LCL95\": prr_l, \"PRR_UCL95\": prr_u,\n",
    "        \"ROR\": ror, \"ROR_LCL95\": ror_l, \"ROR_UCL95\": ror_u,\n",
    "        \"test\": test, \"stat\": stat, \"p_value\": p\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Benjamini–Hochberg (FDR) across tested pairs ----------\n",
    "def bh_fdr(pvals, alpha=0.05):\n",
    "    p = np.array([pv if pd.notna(pv) else 1.0 for pv in pvals], dtype=float)\n",
    "    m = len(p)\n",
    "    order = np.argsort(p)\n",
    "    ranked = p[order]\n",
    "    thresh = alpha * (np.arange(1, m+1) / m)\n",
    "    passed = ranked <= thresh\n",
    "    crit = np.max(np.where(passed)[0]) if passed.any() else -1\n",
    "    sig = np.zeros(m, dtype=bool)\n",
    "    if crit >= 0:\n",
    "        sig[order[:crit+1]] = True\n",
    "    return sig\n",
    "\n",
    "if len(res):\n",
    "    res[\"BH_significant_0.05\"] = bh_fdr(res[\"p_value\"].values, alpha=0.05)\n",
    "\n",
    "res.to_csv(OUT_CSV, index=False)\n",
    "display(res)\n",
    "print(\"Saved ->\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9569c00-f679-4659-8115-3ec2269bb983",
   "metadata": {},
   "source": [
    "This cell visualizes time trends for a chosen brand×problem pair and, when feasible, fits an Interrupted Time Series (ITS) around the 2024-Q1 breakpoint. It first normalizes brand names (simple regex cleanup) and matches the target brand to the canonical label used in the unified data. Next, it builds a quarterly series: total MAUDE events (denominator) and the brand×problem events (numerator), then computes proportions. If we have enough pre/post quarters (≈≥6 each), it fits a segmented regression and plots observed points, fitted lines, and a counterfactual. If data are sparse, it falls back to a clean proportion-only plot. All figures and the model coefficients (when ITS runs) are saved to outputs/figures and outputs/tables for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40b84e-30fb-4ea3-9c57-10420b7baf58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Cell 07 (robust): quarterly proportions with brand-only fallback ===\n",
    "import os, re, unicodedata\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_FIG  = OUT_ROOT / \"figures\"\n",
    "OUT_FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOW_START  = \"2021-Q1\"\n",
    "WINDOW_END    = \"2024-Q4\"\n",
    "BREAK_QUARTER = \"2024-Q1\"\n",
    "TARGET_INDEX  = 0  # 0 or 1 from targets_selected.csv\n",
    "\n",
    "df_path  = OUT_TAB / \"_analysis_event_level.csv\"\n",
    "qtot_path= OUT_TAB / \"quarter_totals.csv\"\n",
    "tgt_path = OUT_TAB / \"targets_selected.csv\"\n",
    "assert df_path.exists() and tgt_path.exists(), \"Missing _analysis_event_level.csv or targets_selected.csv\"\n",
    "\n",
    "df      = pd.read_csv(df_path, dtype=str, usecols=[\"event_id\",\"brand\",\"problem_code\",\"quarter\"])\n",
    "targets = pd.read_csv(tgt_path).to_dict(\"records\")\n",
    "\n",
    "brand_raw = targets[TARGET_INDEX][\"brand\"]\n",
    "code_raw  = str(targets[TARGET_INDEX][\"problem_code\"])\n",
    "\n",
    "print(f\"Requested target → {brand_raw} × {code_raw}\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def canon_brand(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s)).upper()\n",
    "    s = re.sub(r\"[^A-Z0-9 ]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def qord(q: str) -> int:\n",
    "    y, s = q.split(\"-Q\"); return int(y)*4 + int(s)\n",
    "\n",
    "def normalize_quarter(q):\n",
    "    if not isinstance(q, str): return np.nan\n",
    "    q = q.strip().upper().replace(\" \", \"\")\n",
    "    m = re.search(r\"(\\d{4}).*?Q([1-4])\", q)\n",
    "    return f\"{m.group(1)}-Q{m.group(2)}\" if m else np.nan\n",
    "\n",
    "# ---------- canonicalize + window ----------\n",
    "df[\"brand_canon\"] = df[\"brand\"].map(canon_brand)\n",
    "df[\"problem_code\"] = df[\"problem_code\"].astype(str).str.strip()\n",
    "df[\"quarter\"] = df[\"quarter\"].map(normalize_quarter)\n",
    "\n",
    "# drop bad quarters\n",
    "df = df[df[\"quarter\"].notna()].copy()\n",
    "\n",
    "# window bounds\n",
    "lo, hi = qord(WINDOW_START), qord(WINDOW_END)\n",
    "df = df[(df[\"quarter\"].map(qord) >= lo) & (df[\"quarter\"].map(qord) <= hi)]\n",
    "\n",
    "# ---------- denominator (quarter totals) ----------\n",
    "use_qtot = None\n",
    "if qtot_path.exists():\n",
    "    qt = pd.read_csv(qtot_path)\n",
    "    # be liberal: accept any col name that looks like quarter/total\n",
    "    qcol = next((c for c in qt.columns if re.search(r\"quarter\", c, re.I)), None)\n",
    "    tcol = next((c for c in qt.columns if re.search(r\"total\",   c, re.I)), None)\n",
    "    if qcol and tcol:\n",
    "        qt = qt[[qcol, tcol]].rename(columns={qcol:\"quarter\", tcol:\"total\"})\n",
    "        qt[\"quarter\"] = qt[\"quarter\"].map(normalize_quarter)\n",
    "        qt = qt[qt[\"quarter\"].notna()].copy()\n",
    "        qt = qt[(qt[\"quarter\"].map(qord) >= lo) & (qt[\"quarter\"].map(qord) <= hi)]\n",
    "        if len(qt):\n",
    "            use_qtot = qt\n",
    "\n",
    "if use_qtot is None:\n",
    "    # fallback: compute totals directly from df\n",
    "    use_qtot = (df.groupby(\"quarter\", as_index=False)[\"event_id\"]\n",
    "                  .count().rename(columns={\"event_id\":\"total\"}))\n",
    "\n",
    "# ---------- choose brand (exact or best candidate) ----------\n",
    "brand_target = canon_brand(brand_raw)\n",
    "unique_brands = df[\"brand_canon\"].value_counts()\n",
    "\n",
    "if brand_target in unique_brands.index:\n",
    "    brand_use = brand_target\n",
    "    print(\"✓ Exact canonical brand match found.\")\n",
    "else:\n",
    "    print(\"⚠️ Exact brand not found; picking nearest by token overlap.\")\n",
    "    tokens = [t for t in brand_target.split(\" \") if t]\n",
    "    cand = unique_brands.rename(\"n\").rename_axis(\"brand_canon\").reset_index()\n",
    "    def score(bc: str) -> int:\n",
    "        bcsp = f\" {bc} \"\n",
    "        return sum(1 for t in tokens if f\" {t} \" in bcsp)\n",
    "    cand[\"hits\"] = cand[\"brand_canon\"].map(score)\n",
    "    if len(cand) and cand[\"hits\"].max() > 0:\n",
    "        cand = cand.sort_values([\"hits\",\"n\"], ascending=[False,False])\n",
    "        brand_use = cand.iloc[0][\"brand_canon\"]\n",
    "        print(\"→ Auto-selected:\", brand_use)\n",
    "        print(cand.head(5))\n",
    "    else:\n",
    "        print(\"❌ No viable candidate; plotting zeros.\")\n",
    "        brand_use = brand_target\n",
    "\n",
    "# ---------- numerator selection (brand×code, else brand-only fallback) ----------\n",
    "mask_brand = (df[\"brand_canon\"] == brand_use)\n",
    "\n",
    "# If problem_code is basically missing in this build, fallback to brand-only\n",
    "code_present = df[\"problem_code\"].notna().sum() > 0 and (df[\"problem_code\"] != \"None\").sum() > 0\n",
    "use_code = code_present and (df.loc[mask_brand, \"problem_code\"] == code_raw).any()\n",
    "\n",
    "if use_code:\n",
    "    print(\"Numerator mode: brand × problem_code\")\n",
    "    mask_num = mask_brand & (df[\"problem_code\"] == code_raw)\n",
    "else:\n",
    "    print(\"Numerator mode: brand-only (problem_code absent or not joined in this build)\")\n",
    "    mask_num = mask_brand\n",
    "\n",
    "q_num = (df.loc[mask_num]\n",
    "           .groupby(\"quarter\", as_index=False)[\"event_id\"]\n",
    "           .count().rename(columns={\"event_id\":\"num\"}))\n",
    "\n",
    "ts = use_qtot.merge(q_num, on=\"quarter\", how=\"left\").fillna({\"num\":0})\n",
    "ts[\"prop\"] = ts[\"num\"] / ts[\"total\"]\n",
    "ts = ts.sort_values(\"quarter\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nDiagnostics:\")\n",
    "print(f\"- Using brand_canon: {brand_use}\")\n",
    "print(f\"- problem_code available? {code_present} | using code match? {use_code}\")\n",
    "print(f\"- numerator sum: {int(ts['num'].sum()):,} | denominator sum: {int(ts['total'].sum()):,}\")\n",
    "print(\"First few rows:\\n\", ts.head())\n",
    "\n",
    "# ---------- plot ----------\n",
    "plt.figure(figsize=(8,4.5))\n",
    "label = \"Observed proportion\" + (\"\" if use_code else \" (brand-only)\")\n",
    "plt.plot(ts[\"quarter\"], ts[\"prop\"], marker=\"o\", linewidth=1.5, label=label)\n",
    "\n",
    "bpt = qord(BREAK_QUARTER)\n",
    "if ts[\"quarter\"].map(qord).between(lo, hi).any() and (ts[\"quarter\"].map(qord)==bpt).any():\n",
    "    qlab = ts.loc[ts[\"quarter\"].map(qord)==bpt, \"quarter\"].iloc[0]\n",
    "    plt.axvline(qlab, ls=\"--\", alpha=0.6, label=f\"Break: {BREAK_QUARTER}\")\n",
    "\n",
    "title_suffix = f\"{brand_raw}\" if not use_code else f\"{brand_raw} × {code_raw}\"\n",
    "plt.title(f\"{title_suffix}\\nQuarterly proportion of all events ({WINDOW_START}–{WINDOW_END})\")\n",
    "plt.xlabel(\"Quarter\"); plt.ylabel(\"Proportion\"); plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(alpha=0.25); plt.legend(loc=\"upper left\"); plt.tight_layout()\n",
    "\n",
    "def safe(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\",\"_\", s)\n",
    "\n",
    "fname = f\"{safe(brand_raw)}_{safe(code_raw if use_code else 'BRAND_ONLY')}_prop_{WINDOW_START}_{WINDOW_END}.png\"\n",
    "out_png = OUT_FIG / fname\n",
    "out_csv = OUT_TAB / (fname.replace(\".png\",\".csv\"))\n",
    "\n",
    "plt.savefig(out_png, dpi=150); plt.show()\n",
    "ts.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", out_png)\n",
    "print(\" -\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de2313-e131-42fc-9ca0-8d310c41a0d4",
   "metadata": {},
   "source": [
    "This cell gives a quick, audience-friendly summary of effect size. For the selected brand×problem, it builds a quarterly series from the unified dataset, labels each quarter as Pre or Post relative to the 2024-Q1 breakpoint, and computes—per period—the number of quarters, total events for the pair (numerator), overall MAUDE events (denominator), and the mean proportion (events/total). It then exports a tidy CSV (pre_post_summary_*.csv) for tables and renders a simple bar chart comparing Pre vs Post mean proportions. Use this to complement ITS: even when formal modeling is underpowered, this view communicates direction and magnitude cleanly for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0871d9-d896-4abb-a071-10e13adf0170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Cell 08A: Pre/Post summary computation ===\n",
    "# Loads unified event data + quarter totals, applies window filtering,\n",
    "# computes pre/post proportions for the selected brand × problem_code target.\n",
    "\n",
    "import os, re, unicodedata\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- paths/params ----\n",
    "RAW_DIR  = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser()\n",
    "OUT_ROOT = RAW_DIR.parent.parent / \"outputs\"\n",
    "OUT_TAB  = OUT_ROOT / \"tables\"\n",
    "OUT_FIG  = OUT_ROOT / \"figures\"\n",
    "\n",
    "WINDOW_START  = \"2021-Q1\"\n",
    "WINDOW_END    = \"2024-Q4\"\n",
    "BREAK_QUARTER = \"2024-Q1\"\n",
    "TARGET_INDEX  = 0  # 0 or 1 from targets_selected.csv\n",
    "\n",
    "# ---- load inputs ----\n",
    "df_path   = OUT_TAB / \"_analysis_event_level.csv\"\n",
    "qtot_path = OUT_TAB / \"quarter_totals.csv\"\n",
    "tgt_path  = OUT_TAB / \"targets_selected.csv\"\n",
    "assert df_path.exists() and qtot_path.exists() and tgt_path.exists(), \"Missing required inputs.\"\n",
    "\n",
    "df      = pd.read_csv(df_path, dtype=str, usecols=[\"event_id\",\"brand\",\"problem_code\",\"quarter\"])\n",
    "targets = pd.read_csv(tgt_path).to_dict(\"records\")\n",
    "qt      = pd.read_csv(qtot_path)\n",
    "\n",
    "brand_raw = targets[TARGET_INDEX][\"brand\"]\n",
    "code_raw  = str(targets[TARGET_INDEX][\"problem_code\"])\n",
    "print(f\"Pre/Post summary for → {brand_raw} × {code_raw}\")\n",
    "\n",
    "# ---- helpers ----\n",
    "def canon_brand(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s) if s is not None else \"\")\n",
    "    s = s.upper()\n",
    "    s = re.sub(r\"[^A-Z0-9 ]+\",\" \", s)\n",
    "    return re.sub(r\"\\s+\",\" \", s).strip()\n",
    "\n",
    "def normalize_quarter(q):\n",
    "    if not isinstance(q, str): return np.nan\n",
    "    q = q.strip().upper().replace(\" \", \"\")\n",
    "    m = re.search(r\"(\\d{4}).*?Q([1-4])\", q)\n",
    "    return f\"{m.group(1)}-Q{m.group(2)}\" if m else np.nan\n",
    "\n",
    "def qord(q: str) -> int:\n",
    "    y, s = q.split(\"-Q\"); return int(y)*4 + int(s)\n",
    "\n",
    "def safe(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\",\"_\", s)\n",
    "\n",
    "# ---- canonicalize + filter ----\n",
    "df[\"brand_canon\"]   = df[\"brand\"].map(canon_brand)\n",
    "df[\"problem_code\"]  = df[\"problem_code\"].astype(str).str.strip()\n",
    "df[\"quarter\"]       = df[\"quarter\"].map(normalize_quarter)\n",
    "df                  = df[df[\"quarter\"].notna()].copy()\n",
    "\n",
    "lo, hi = qord(WINDOW_START), qord(WINDOW_END)\n",
    "df = df[(df[\"quarter\"].map(qord) >= lo) & (df[\"quarter\"].map(qord) <= hi)]\n",
    "\n",
    "# ---- quarter totals ----\n",
    "qcol = next((c for c in qt.columns if re.search(r\"quarter\", c, re.I)), None)\n",
    "tcol = next((c for c in qt.columns if re.search(r\"total\",   c, re.I)), None)\n",
    "qt = qt[[qcol, tcol]].rename(columns={qcol:\"quarter\", tcol:\"total\"})\n",
    "qt[\"quarter\"] = qt[\"quarter\"].map(normalize_quarter)\n",
    "qt = qt[qt[\"quarter\"].notna()]\n",
    "qt = qt[(qt[\"quarter\"].map(qord) >= lo) & (qt[\"quarter\"].map(qord) <= hi)]\n",
    "\n",
    "# ---- brand + problem selection ----\n",
    "brand_target = canon_brand(brand_raw)\n",
    "unique_brands = df[\"brand_canon\"].value_counts()\n",
    "brand_use = brand_target if brand_target in unique_brands.index else brand_target\n",
    "mask_brand = (df[\"brand_canon\"] == brand_use)\n",
    "\n",
    "code_present = df[\"problem_code\"].notna().sum() > 0 and (df[\"problem_code\"] != \"None\").sum() > 0\n",
    "use_code = code_present and (df.loc[mask_brand, \"problem_code\"] == code_raw).any()\n",
    "\n",
    "if use_code:\n",
    "    mask_num = mask_brand & (df[\"problem_code\"] == code_raw)\n",
    "    label_suffix = f\"{brand_raw} × {code_raw}\"\n",
    "else:\n",
    "    mask_num = mask_brand\n",
    "    label_suffix = f\"{brand_raw} (brand-only)\"\n",
    "    print(\"Note: problem_code missing; using brand-only fallback.\")\n",
    "\n",
    "# ---- compute quarterly proportions ----\n",
    "q_num = (df.loc[mask_num]\n",
    "           .groupby(\"quarter\", as_index=False)[\"event_id\"]\n",
    "           .count().rename(columns={\"event_id\":\"num\"}))\n",
    "ts = qt.merge(q_num, on=\"quarter\", how=\"left\").fillna({\"num\":0})\n",
    "ts[\"prop\"] = ts[\"num\"] / ts[\"total\"]\n",
    "ts = ts.sort_values(\"quarter\").reset_index(drop=True)\n",
    "\n",
    "# ---- pre/post summary ----\n",
    "bpt = qord(BREAK_QUARTER)\n",
    "ts[\"period\"] = np.where(ts[\"quarter\"].map(qord) >= bpt, \"Post\", \"Pre\")\n",
    "summary = (ts.groupby(\"period\", as_index=False)\n",
    "             .agg(quarters=(\"quarter\",\"nunique\"),\n",
    "                  events=(\"num\",\"sum\"),\n",
    "                  total=(\"total\",\"sum\"),\n",
    "                  mean_prop=(\"prop\",\"mean\"))\n",
    "             .sort_values(\"period\"))\n",
    "\n",
    "# ---- save summary ----\n",
    "csv_name = f\"pre_post_summary_{safe(label_suffix)}_{WINDOW_START}_{WINDOW_END}.csv\"\n",
    "summary_path = OUT_TAB / csv_name\n",
    "summary.to_csv(summary_path, index=False)\n",
    "display(summary)\n",
    "print(\"Saved ->\", summary_path)\n",
    "\n",
    "# store variables for Cell 8B\n",
    "_ = (label_suffix, summary_path, summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3df95-e898-4eb8-a847-54c7e5370396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Cell 08B: Visualization (Pre vs Post Bar Chart) ===\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# assumes variables from 8A: label_suffix, summary, WINDOW_START/END\n",
    "def safe(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\",\"_\", s)\n",
    "\n",
    "OUT_ROOT = Path(os.environ.get(\"MAUDE_RAW\",\"\")).expanduser().parent.parent / \"outputs\"\n",
    "OUT_FIG  = OUT_ROOT / \"figures\"\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(summary[\"period\"], summary[\"mean_prop\"])\n",
    "plt.title(f\"Pre vs Post Mean Proportion\\n{label_suffix} ({WINDOW_START}–{WINDOW_END})\")\n",
    "plt.ylabel(\"Mean proportion of all events\")\n",
    "for i, v in enumerate(summary[\"mean_prop\"]):\n",
    "    plt.text(i, v, f\"{v:.3%}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "\n",
    "png_name = f\"pre_post_bar_{safe(label_suffix)}_{WINDOW_START}_{WINDOW_END}.png\"\n",
    "fig_path = OUT_FIG / png_name\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "display(Image(filename=str(fig_path)))\n",
    "print(\"Saved ->\", fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d36a94-603f-4842-b2f2-a9920b5d41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Export: manufacturer-level MAUDE safety summary\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Adjust this to your main event-level CSV if different\n",
    "EVENTS_PATH = Path(\"data/derived/analysis_event_level.csv\")\n",
    "df_events = pd.read_csv(EVENTS_PATH)\n",
    "\n",
    "def normalize_name(name: str) -> str | None:\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    return \" \".join(str(name).upper().split())\n",
    "\n",
    "# Ensure we have a normalized manufacturer column\n",
    "if \"manufacturer_normalized\" not in df_events.columns:\n",
    "    if \"manufacturer_name\" in df_events.columns:\n",
    "        df_events[\"manufacturer_normalized\"] = df_events[\"manufacturer_name\"].apply(normalize_name)\n",
    "    elif \"manufacturer\" in df_events.columns:\n",
    "        df_events[\"manufacturer_normalized\"] = df_events[\"manufacturer\"].apply(normalize_name)\n",
    "    else:\n",
    "        raise KeyError(\"Need a manufacturer column to normalize.\")\n",
    "\n",
    "summary = (\n",
    "    df_events\n",
    "    .groupby(\"manufacturer_normalized\")\n",
    "    .agg(\n",
    "        total_events=(\"event_id\", \"nunique\"),     # change names here if needed\n",
    "        serious_events=(\"serious_flag\", \"sum\"),\n",
    "        deaths=(\"death_flag\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary[\"safety_score\"] = (\n",
    "    0.6 * (summary[\"serious_events\"] / summary[\"total_events\"].clip(lower=1))\n",
    "    + 0.4 * (summary[\"deaths\"] / summary[\"total_events\"].clip(lower=1))\n",
    ")\n",
    "\n",
    "out_path = Path(\"data/derived/maude_safety_summary.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary.to_csv(out_path, index=False)\n",
    "print(\"Wrote\", out_path, \"with shape\", summary.shape)\n",
    "summary.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad2f36-725e-4c4f-b63a-7c6fc9387c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417f707-0063-432e-8e30-87713450e987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (maude)",
   "language": "python",
   "name": "maude"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
